<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Linear Regression with One Regressor | Introduction to Econometrics with R</title>
  <meta name="description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programming and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Linear Regression with One Regressor | Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://www.econometrics-with-r.org//images/cover.png" />
  <meta property="og:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programming and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="github-repo" content="mca91/EconometricsWithR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Linear Regression with One Regressor | Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programming and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="twitter:image" content="https://www.econometrics-with-r.org//images/cover.png" />

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer" />


<meta name="date" content="2024-12-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="arosur.html"/>
<link rel="next" href="htaciitslrm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.3/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><center><img src="images/logo.png" alt="logo" width="50%" height="50%"style="margin: 15px 0 0 0"></center></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#colophon"><i class="fa fa-check"></i><b>1.1</b> Colophon</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#a-very-short-introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.2</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="2.1" data-path="pt.html"><a href="pt.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a>
<ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pt.html"><a href="pt.html#RSATDOSA"><i class="fa fa-check"></i><b>2.2</b> Random Sampling and the Distribution of Sample Averages</a>
<ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pt.html"><a href="pt.html#exercises-2"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arosur.html"><a href="arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="arosur.html"><a href="arosur.html#estimation-of-the-population-mean"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="arosur.html"><a href="arosur.html#potsm"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="arosur.html"><a href="arosur.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests concerning the Population Mean</a>
<ul>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="arosur.html"><a href="arosur.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="arosur.html"><a href="arosur.html#cmfdp"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="arosur.html"><a href="arosur.html#aattggoe"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="arosur.html"><a href="arosur.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="arosur.html"><a href="arosur.html#exercises-3"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lrwor.html"><a href="lrwor.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a>
<ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a>
<ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lrwor.html"><a href="lrwor.html#tlsa"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a>
<ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a>
<ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="lrwor.html"><a href="lrwor.html#exercises-4"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="htaciitslrm.html"><a href="htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in SLR Model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="htaciitslrm.html"><a href="htaciitslrm.html#testing-two-sided-hypotheses-concerning-the-slope-coefficient"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="htaciitslrm.html"><a href="htaciitslrm.html#cifrc"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a>
<ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="htaciitslrm.html"><a href="htaciitslrm.html#rwxiabv"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="htaciitslrm.html"><a href="htaciitslrm.html#hah"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a>
<ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="htaciitslrm.html"><a href="htaciitslrm.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a>
<ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="htaciitslrm.html"><a href="htaciitslrm.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression when the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="htaciitslrm.html"><a href="htaciitslrm.html#exercises-5"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rmwmr.html"><a href="rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a>
<ul>
<li class="chapter" data-level="6.1" data-path="rmwmr.html"><a href="rmwmr.html#omitted-variable-bias"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="rmwmr.html"><a href="rmwmr.html#tmrm"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="rmwmr.html"><a href="rmwmr.html#mofimr"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="rmwmr.html"><a href="rmwmr.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a>
<ul>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rmwmr.html"><a href="rmwmr.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="rmwmr.html"><a href="rmwmr.html#exercises-6"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="htaciimr.html"><a href="htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in MR Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="htaciimr.html"><a href="htaciimr.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="htaciimr.html"><a href="htaciimr.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a>
<ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="htaciimr.html"><a href="htaciimr.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing using the F-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="htaciimr.html"><a href="htaciimr.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a>
<ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="htaciimr.html"><a href="htaciimr.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="htaciimr.html"><a href="htaciimr.html#exercises-7"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nrf.html"><a href="nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="nrf.html"><a href="nrf.html#a-general-strategy-for-modelling-nonlinear-regression-functions"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nrf.html"><a href="nrf.html#nfoasiv"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a>
<ul>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="nrf.html"><a href="nrf.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.3</b> Interactions between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nrf.html"><a href="nrf.html#nonlinear-effects-on-test-scores-of-the-student-teacher-ratio"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="8.5" data-path="nrf.html"><a href="nrf.html#exercises-8"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="asbomr.html"><a href="asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="asbomr.html"><a href="asbomr.html#ttivomra"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity-when-the-regression-is-used-for-forecasting"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity when the Regression is used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="asbomr.html"><a href="asbomr.html#etsacs"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="9.5" data-path="asbomr.html"><a href="asbomr.html#exercises-9"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rwpd.html"><a href="rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a>
<ul>
<li class="chapter" data-level="10.1" data-path="rwpd.html"><a href="rwpd.html#panel-data"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="rwpd.html"><a href="rwpd.html#PDWTTP"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="rwpd.html"><a href="rwpd.html#fixed-effects-regression"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a>
<ul>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="rwpd.html"><a href="rwpd.html#regression-with-time-fixed-effects"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="rwpd.html"><a href="rwpd.html#tferaaseffer"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="rwpd.html"><a href="rwpd.html#drunk-driving-laws-and-traffic-deaths"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
<li class="chapter" data-level="10.7" data-path="rwpd.html"><a href="rwpd.html#exercises-10"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rwabdv.html"><a href="rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a>
<ul>
<li class="chapter" data-level="11.1" data-path="rwabdv.html"><a href="rwabdv.html#binary-dependent-variables-and-the-linear-probability-model"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="rwabdv.html"><a href="rwabdv.html#palr"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a>
<ul>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="rwabdv.html"><a href="rwabdv.html#estimation-and-inference-in-the-logit-and-probit-models"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="rwabdv.html"><a href="rwabdv.html#application-to-the-boston-hmda-data"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="11.5" data-path="rwabdv.html"><a href="rwabdv.html#exercises-11"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ivr.html"><a href="ivr.html#TIVEWASRAASI"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="ivr.html"><a href="ivr.html#TGIVRM"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="ivr.html"><a href="ivr.html#civ"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="ivr.html"><a href="ivr.html#attdfc"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="ivr.html"><a href="ivr.html#where-do-valid-instruments-come-from"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="12.6" data-path="ivr.html"><a href="ivr.html#exercises-12"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="eaqe.html"><a href="eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a>
<ul>
<li class="chapter" data-level="13.1" data-path="eaqe.html"><a href="eaqe.html#poceaie"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="eaqe.html"><a href="eaqe.html#threats-to-validity-of-experiments"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="eaqe.html"><a href="eaqe.html#experimental-estimates-of-the-effect-of-class-size-reductions"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a>
<ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="eaqe.html"><a href="eaqe.html#qe"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a>
<ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="eaqe.html"><a href="eaqe.html#exercises-13"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ittsraf.html"><a href="ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ittsraf.html"><a href="ittsraf.html#using-regression-models-for-forecasting"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="ittsraf.html"><a href="ittsraf.html#tsdasc"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a>
<ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ittsraf.html"><a href="ittsraf.html#autoregressions"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a>
<ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#autoregressive-models-of-order-p"><i class="fa fa-check"></i>Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ittsraf.html"><a href="ittsraf.html#cybtmpi"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="ittsraf.html"><a href="ittsraf.html#apatadlm"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a>
<ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ittsraf.html"><a href="ittsraf.html#llsuic"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="ittsraf.html"><a href="ittsraf.html#nit"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="ittsraf.html"><a href="ittsraf.html#niib"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="ittsraf.html"><a href="ittsraf.html#can-you-beat-the-market-part-ii"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="eodce.html"><a href="eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a>
<ul>
<li class="chapter" data-level="15.1" data-path="eodce.html"><a href="eodce.html#the-orange-juice-data"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="eodce.html"><a href="eodce.html#dynamic-causal-effects"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="eodce.html"><a href="eodce.html#dynamic-multipliers-and-cumulative-dynamic-multipliers"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="eodce.html"><a href="eodce.html#hac-standard-errors"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="eodce.html"><a href="eodce.html#estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="eodce.html"><a href="eodce.html#orange-juice-prices-and-cold-weather"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="atitsr.html"><a href="atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="atitsr.html"><a href="atitsr.html#vector-autoregressions"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="atitsr.html"><a href="atitsr.html#ooiatdfglsurt"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="atitsr.html"><a href="atitsr.html#cointegration"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="atitsr.html"><a href="atitsr.html#volatility-clustering-and-autoregressive-conditional-heteroskedasticity"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a>
<ul>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lrwor" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Linear Regression with One Regressor<a href="lrwor.html#lrwor" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter introduces the basics in linear regression and shows how to perform regression analysis in <tt>R</tt>. In linear regression, the aim is to model the relationship between a dependent variable <span class="math inline">\(Y\)</span> and one or more explanatory variables denoted by <span class="math inline">\(X_1, X_2, \dots, X_k\)</span>. Following the book we will focus on the concept of simple linear regression throughout the whole chapter. In simple linear regression, there is just one explanatory variable <span class="math inline">\(X_1\)</span>. <br>
If, for example, a school cuts its class sizes by hiring new teachers, that is, the school lowers <span class="math inline">\(X_1\)</span>, the student-teacher ratios of its classes, how would this affect <span class="math inline">\(Y\)</span>, the performance of the students involved in a standardized test? With linear regression we can not only examine whether the student-teacher ratio <em>does have</em> an impact on the test results but we can also learn about the <em>direction</em> and the <em>strength</em> of this effect.</p>
<p>The following packages are needed for reproducing the code presented in this chapter:</p>
<ul>
<li><p><tt>AER</tt> - accompanies the Book <em>Applied Econometrics with R</em> <span class="citation">C. Kleiber and Zeileis (<a href="#ref-kleiber2008">2008</a>)</span> and provides useful functions and data sets.</p></li>
<li><p><tt>MASS</tt> - a collection of functions for applied statistics.</p></li>
</ul>
<p>Make sure these are installed before you go ahead and try to replicate the examples. The safest way to do so is by checking whether the following code chunk executes without any errors.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="lrwor.html#cb80-1" tabindex="-1"></a><span class="fu">library</span>(AER)</span>
<span id="cb80-2"><a href="lrwor.html#cb80-2" tabindex="-1"></a><span class="fu">library</span>(MASS)</span></code></pre></div>
<div id="simple-linear-regression" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Simple Linear Regression<a href="lrwor.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To start with an easy example, consider the following combinations of average test score and the average student-teacher ratio in some fictional school districts.</p>
<p>To work with these data in <tt>R</tt> we begin by generating two vectors: one for the student-teacher ratios (<tt>STR</tt>) and one for test scores (<tt>TestScore</tt>), both containing the data from the table above.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="lrwor.html#cb81-1" tabindex="-1"></a><span class="co"># Create sample data</span></span>
<span id="cb81-2"><a href="lrwor.html#cb81-2" tabindex="-1"></a>STR <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">15</span>, <span class="dv">17</span>, <span class="dv">19</span>, <span class="dv">20</span>, <span class="dv">22</span>, <span class="fl">23.5</span>, <span class="dv">25</span>)</span>
<span id="cb81-3"><a href="lrwor.html#cb81-3" tabindex="-1"></a>TestScore <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">680</span>, <span class="dv">640</span>, <span class="dv">670</span>, <span class="dv">660</span>, <span class="dv">630</span>, <span class="dv">660</span>, <span class="dv">635</span>) </span>
<span id="cb81-4"><a href="lrwor.html#cb81-4" tabindex="-1"></a></span>
<span id="cb81-5"><a href="lrwor.html#cb81-5" tabindex="-1"></a><span class="co"># Print out sample data</span></span>
<span id="cb81-6"><a href="lrwor.html#cb81-6" tabindex="-1"></a>STR</span>
<span id="cb81-7"><a href="lrwor.html#cb81-7" tabindex="-1"></a><span class="co">#&gt; [1] 15.0 17.0 19.0 20.0 22.0 23.5 25.0</span></span>
<span id="cb81-8"><a href="lrwor.html#cb81-8" tabindex="-1"></a>TestScore</span>
<span id="cb81-9"><a href="lrwor.html#cb81-9" tabindex="-1"></a><span class="co">#&gt; [1] 680 640 670 660 630 660 635</span></span></code></pre></div>
<p>To build simple linear regression model, we hypothesize that the relationship between dependent and independent variable is linear, formally: <span class="math display">\[ Y = b \cdot X + a. \]</span> For now, let us suppose that the function which relates test score and student-teacher ratio
to each other is <span class="math display">\[TestScore = 713 - 3 \times STR.\]</span></p>
<p>It is always a good idea to visualize the data you work with. Here, it is suitable to use <tt>plot()</tt> to produce a scatterplot with <tt>STR</tt> on the <span class="math inline">\(x\)</span>-axis and <tt>TestScore</tt> on the <span class="math inline">\(y\)</span>-axis. Just call <code>plot(y_variable ~ x_variable)</code> whereby <tt>y_variable</tt> and <tt>x_variable</tt> are placeholders for the vectors of observations we want to plot. Furthermore, we might want to add a systematic relationship to the plot. To draw a straight line, <tt>R</tt> provides the function <tt>abline()</tt>. We just have to call this function with arguments <tt>a</tt> (representing the intercept)
and <tt>b</tt> (representing the slope) after executing <tt>plot()</tt> in order to add the line to our plot.</p>
<p>The following code reproduces Figure 4.1 from the textbook.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="lrwor.html#cb82-1" tabindex="-1"></a><span class="co"># create a scatterplot of the data</span></span>
<span id="cb82-2"><a href="lrwor.html#cb82-2" tabindex="-1"></a><span class="fu">plot</span>(TestScore <span class="sc">~</span> STR,<span class="at">ylab=</span><span class="st">&quot;Test Score&quot;</span>,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb82-3"><a href="lrwor.html#cb82-3" tabindex="-1"></a></span>
<span id="cb82-4"><a href="lrwor.html#cb82-4" tabindex="-1"></a><span class="co"># add the systematic relationship to the plot</span></span>
<span id="cb82-5"><a href="lrwor.html#cb82-5" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">713</span>, <span class="at">b =</span> <span class="sc">-</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-146-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>We find that the line does not touch any of the points although we claimed that it represents the systematic relationship. The reason for this is randomness. Most of the time there are additional influences which imply that there is no bivariate relationship between the two variables.</p>
<p>In order to account for these differences between observed data and the systematic relationship, we extend our model from above by an <em>error term</em> <span class="math inline">\(u\)</span> which captures additional random effects. Put differently, <span class="math inline">\(u\)</span> accounts for all the differences between the regression line and the actual observed data. Beside pure randomness, these deviations could also arise from measurement errors or, as will be discussed later, could be the consequence of leaving out other factors that are relevant in explaining the dependent variable.</p>
<p>Which other factors are plausible in our example? For one thing, the test scores might be driven by the teachers’ quality and the background of the students. It is also possible that in some classes, the students were lucky on the test days and thus achieved higher scores. For now, we will summarize such influences by an additive component:</p>
<p><span class="math display">\[ TestScore = \beta_0 + \beta_1 \times STR + \text{other factors}. \]</span></p>
<p>Of course this idea is very general as it can be easily extended to other situations that can be described with a linear model. Hence, the basic linear regression model we will work with is</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + u_i. \]</span></p>
<p>Key Concept 4.1 summarizes the linear regression model and its terminology.</p>
<div id="KC4.1" class="keyconcept">
<h3 class="right">
Key Concept 4.1
</h3>
<h3 class="left">
Terminology for the Linear Regression Model with a Single Regressor
</h3>
<p>
<p>The linear regression model is</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i,\]</span></p>
<p>where</p>
<ul>
<li>the index <span class="math inline">\(i\)</span> runs over the observations, <span class="math inline">\(i=1,\dots,n\)</span>;</li>
<li><span class="math inline">\(Y_i\)</span> is the <em>dependent variable</em>, the <em>regressand</em>, or simply the <em>left-hand variable</em>;</li>
<li><span class="math inline">\(X_i\)</span> is the <em>independent variable</em>, the <em>regressor</em>, or simply the <em>right-hand variable</em>;</li>
<li><span class="math inline">\(Y = \beta_0 + \beta_1 X\)</span> is the <em>population regression line</em> also called the <em>population regression function</em>;</li>
<li><span class="math inline">\(\beta_0\)</span> is the <em>intercept</em> of the population regression line;</li>
<li><span class="math inline">\(\beta_1\)</span> is the <em>slope</em> of the population regression line;</li>
<li><span class="math inline">\(u_i\)</span> is the <em>error term</em>.</li>
</ul>
</p>
</div>
</div>
<div id="estimating-the-coefficients-of-the-linear-regression-model" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Estimating the Coefficients of the Linear Regression Model<a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In practice, the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span> of the population regression line are unknown. Therefore, we must employ data to estimate both unknown parameters. In the following, a real world example will be used to demonstrate how this is achieved. We want to relate test scores to student-teacher ratios measured in Californian schools. The test score is the district-wide average of reading and math scores for fifth graders. Again, the class size is measured as the number of students divided by the number of teachers (the student-teacher ratio). As for the data, the California School data set (<tt>CASchools</tt>) comes with an <tt>R</tt> package called <tt>AER</tt>, an acronym for <a href="https://cran.r-project.org/web/packages/AER/AER.pdf">Applied Econometrics with R</a> <span class="citation">(<a href="#ref-R-AER">Christian Kleiber and Zeileis 2008</a>)</span>. After installing the package with <tt>install.packages(“AER”)</tt> and attaching it with <tt>library(AER)</tt> the data set can be loaded using the function <tt>data()</tt>.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="lrwor.html#cb83-1" tabindex="-1"></a><span class="do">## # install the AER package (once)</span></span>
<span id="cb83-2"><a href="lrwor.html#cb83-2" tabindex="-1"></a><span class="do">## install.packages(&quot;AER&quot;)</span></span>
<span id="cb83-3"><a href="lrwor.html#cb83-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb83-4"><a href="lrwor.html#cb83-4" tabindex="-1"></a><span class="do">## # load the AER package</span></span>
<span id="cb83-5"><a href="lrwor.html#cb83-5" tabindex="-1"></a><span class="fu">library</span>(AER)</span>
<span id="cb83-6"><a href="lrwor.html#cb83-6" tabindex="-1"></a></span>
<span id="cb83-7"><a href="lrwor.html#cb83-7" tabindex="-1"></a><span class="co"># load the the data set in the workspace</span></span>
<span id="cb83-8"><a href="lrwor.html#cb83-8" tabindex="-1"></a><span class="fu">data</span>(CASchools)</span></code></pre></div>
<p>Once a package has been installed it is available for use at further occasions when invoked with <tt>library()</tt> — there is no need to run <tt>install.packages()</tt> again!</p>
<p>It is interesting to know what kind of object we are dealing with.
<tt>class()</tt> returns the class of an object. Depending on the class of an object some functions (for example <tt>plot()</tt> and <tt>summary()</tt>) behave differently.</p>
<p>Let us check the class of the object <tt>CASchools</tt>.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="lrwor.html#cb84-1" tabindex="-1"></a><span class="fu">class</span>(CASchools)</span>
<span id="cb84-2"><a href="lrwor.html#cb84-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;data.frame&quot;</span></span></code></pre></div>
<p>It turns out that <tt>CASchools</tt> is of class <tt>data.frame</tt> which is a convenient format to work with, especially for performing regression analysis.</p>
<p>With help of <tt>head()</tt> we get a first overview of our data. This function shows only the first 6 rows of the data set which prevents an overcrowded console output.</p>

<div class="rmdnote">
Press <tt>ctrl + L</tt> to clear the console. This command deletes any code that has been typed in and executed by you or printed to the console by <tt>R</tt> functions. The good news is that anything else is left untouched. You neither lose defined variables etc. nor the code history. It is still possible to recall previously executed <tt>R</tt> commands using the up and down keys. If you are working in <em>RStudio</em>, press <tt>ctrl + Up</tt> on your keyboard (<tt>CMD + Up</tt> on a Mac) to review a list of previously entered commands.
</div>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="lrwor.html#cb85-1" tabindex="-1"></a><span class="fu">head</span>(CASchools)</span>
<span id="cb85-2"><a href="lrwor.html#cb85-2" tabindex="-1"></a><span class="co">#&gt;   district                          school  county grades students teachers</span></span>
<span id="cb85-3"><a href="lrwor.html#cb85-3" tabindex="-1"></a><span class="co">#&gt; 1    75119              Sunol Glen Unified Alameda  KK-08      195    10.90</span></span>
<span id="cb85-4"><a href="lrwor.html#cb85-4" tabindex="-1"></a><span class="co">#&gt; 2    61499            Manzanita Elementary   Butte  KK-08      240    11.15</span></span>
<span id="cb85-5"><a href="lrwor.html#cb85-5" tabindex="-1"></a><span class="co">#&gt; 3    61549     Thermalito Union Elementary   Butte  KK-08     1550    82.90</span></span>
<span id="cb85-6"><a href="lrwor.html#cb85-6" tabindex="-1"></a><span class="co">#&gt; 4    61457 Golden Feather Union Elementary   Butte  KK-08      243    14.00</span></span>
<span id="cb85-7"><a href="lrwor.html#cb85-7" tabindex="-1"></a><span class="co">#&gt; 5    61523        Palermo Union Elementary   Butte  KK-08     1335    71.50</span></span>
<span id="cb85-8"><a href="lrwor.html#cb85-8" tabindex="-1"></a><span class="co">#&gt; 6    62042         Burrel Union Elementary  Fresno  KK-08      137     6.40</span></span>
<span id="cb85-9"><a href="lrwor.html#cb85-9" tabindex="-1"></a><span class="co">#&gt;   calworks   lunch computer expenditure    income   english  read  math</span></span>
<span id="cb85-10"><a href="lrwor.html#cb85-10" tabindex="-1"></a><span class="co">#&gt; 1   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6 690.0</span></span>
<span id="cb85-11"><a href="lrwor.html#cb85-11" tabindex="-1"></a><span class="co">#&gt; 2  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5 661.9</span></span>
<span id="cb85-12"><a href="lrwor.html#cb85-12" tabindex="-1"></a><span class="co">#&gt; 3  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3 650.9</span></span>
<span id="cb85-13"><a href="lrwor.html#cb85-13" tabindex="-1"></a><span class="co">#&gt; 4  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9 643.5</span></span>
<span id="cb85-14"><a href="lrwor.html#cb85-14" tabindex="-1"></a><span class="co">#&gt; 5  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8 639.9</span></span>
<span id="cb85-15"><a href="lrwor.html#cb85-15" tabindex="-1"></a><span class="co">#&gt; 6  12.3188 86.9565       25    5580.147 10.415000 12.408759 605.7 605.4</span></span></code></pre></div>
<p>We find that the data set consists of plenty of variables and that most of them are numeric.</p>
<p>By the way: an alternative to <tt>class()</tt> and <tt>head()</tt> is <tt>str()</tt> which is deduced from ‘structure’ and gives a comprehensive overview of the object. Try!</p>
<p>Turning back to <tt>CASchools</tt>, the two variables we are interested in (i.e., average test score and the student-teacher ratio) are <em>not</em> included. However, it is possible to calculate both from the provided data. To obtain the student-teacher ratios, we simply divide the number of students by the number of teachers. The average test score is the arithmetic mean of the test score for reading and the score of the math test. The next code chunk shows how the two variables can be constructed as vectors and how they are appended to <tt>CASchools</tt>.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="lrwor.html#cb86-1" tabindex="-1"></a><span class="co"># compute STR and append it to CASchools</span></span>
<span id="cb86-2"><a href="lrwor.html#cb86-2" tabindex="-1"></a>CASchools<span class="sc">$</span>STR <span class="ot">&lt;-</span> CASchools<span class="sc">$</span>students<span class="sc">/</span>CASchools<span class="sc">$</span>teachers </span>
<span id="cb86-3"><a href="lrwor.html#cb86-3" tabindex="-1"></a></span>
<span id="cb86-4"><a href="lrwor.html#cb86-4" tabindex="-1"></a><span class="co"># compute TestScore and append it to CASchools</span></span>
<span id="cb86-5"><a href="lrwor.html#cb86-5" tabindex="-1"></a>CASchools<span class="sc">$</span>score <span class="ot">&lt;-</span> (CASchools<span class="sc">$</span>read <span class="sc">+</span> CASchools<span class="sc">$</span>math)<span class="sc">/</span><span class="dv">2</span>     </span></code></pre></div>
<p>If we ran <tt>head(CASchools)</tt> again we would find the two variables of interest as additional columns named <tt>STR</tt> and <tt>score</tt> (check this!).</p>
<p>Table 4.1 from the textbook summarizes the distribution of test scores and student-teacher ratios. There are several functions which can be used to produce similar results, e.g.,</p>
<ul>
<li><p><tt>mean()</tt> (computes the arithmetic mean of the provided numbers),</p></li>
<li><p><tt>sd()</tt> (computes the sample standard deviation),</p></li>
<li><p><tt>quantile()</tt> (returns a vector of the specified sample quantiles for the data).</p></li>
</ul>
<p>The next code chunk shows how to achieve this. First, we compute summary statistics on the columns <tt>STR</tt> and <tt>score</tt> of <tt>CASchools</tt>. In order to get nice output we gather the measures in a <tt>data.frame</tt> named <tt>DistributionSummary</tt>.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="lrwor.html#cb87-1" tabindex="-1"></a><span class="co"># compute sample averages of STR and score</span></span>
<span id="cb87-2"><a href="lrwor.html#cb87-2" tabindex="-1"></a>avg_STR <span class="ot">&lt;-</span> <span class="fu">mean</span>(CASchools<span class="sc">$</span>STR) </span>
<span id="cb87-3"><a href="lrwor.html#cb87-3" tabindex="-1"></a>avg_score <span class="ot">&lt;-</span> <span class="fu">mean</span>(CASchools<span class="sc">$</span>score)</span>
<span id="cb87-4"><a href="lrwor.html#cb87-4" tabindex="-1"></a></span>
<span id="cb87-5"><a href="lrwor.html#cb87-5" tabindex="-1"></a><span class="co"># compute sample standard deviations of STR and score</span></span>
<span id="cb87-6"><a href="lrwor.html#cb87-6" tabindex="-1"></a>sd_STR <span class="ot">&lt;-</span> <span class="fu">sd</span>(CASchools<span class="sc">$</span>STR) </span>
<span id="cb87-7"><a href="lrwor.html#cb87-7" tabindex="-1"></a>sd_score <span class="ot">&lt;-</span> <span class="fu">sd</span>(CASchools<span class="sc">$</span>score)</span>
<span id="cb87-8"><a href="lrwor.html#cb87-8" tabindex="-1"></a></span>
<span id="cb87-9"><a href="lrwor.html#cb87-9" tabindex="-1"></a><span class="co"># set up a vector of percentiles and compute the quantiles </span></span>
<span id="cb87-10"><a href="lrwor.html#cb87-10" tabindex="-1"></a>quantiles <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.10</span>, <span class="fl">0.25</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.75</span>, <span class="fl">0.9</span>)</span>
<span id="cb87-11"><a href="lrwor.html#cb87-11" tabindex="-1"></a>quant_STR <span class="ot">&lt;-</span> <span class="fu">quantile</span>(CASchools<span class="sc">$</span>STR, quantiles)</span>
<span id="cb87-12"><a href="lrwor.html#cb87-12" tabindex="-1"></a>quant_score <span class="ot">&lt;-</span> <span class="fu">quantile</span>(CASchools<span class="sc">$</span>score, quantiles)</span>
<span id="cb87-13"><a href="lrwor.html#cb87-13" tabindex="-1"></a></span>
<span id="cb87-14"><a href="lrwor.html#cb87-14" tabindex="-1"></a><span class="co"># gather everything in a data.frame </span></span>
<span id="cb87-15"><a href="lrwor.html#cb87-15" tabindex="-1"></a>DistributionSummary <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Average =</span> <span class="fu">c</span>(avg_STR, avg_score), </span>
<span id="cb87-16"><a href="lrwor.html#cb87-16" tabindex="-1"></a>                                  <span class="at">StandardDeviation =</span> <span class="fu">c</span>(sd_STR, sd_score), </span>
<span id="cb87-17"><a href="lrwor.html#cb87-17" tabindex="-1"></a>                                  <span class="at">quantile =</span> <span class="fu">rbind</span>(quant_STR, quant_score))</span>
<span id="cb87-18"><a href="lrwor.html#cb87-18" tabindex="-1"></a></span>
<span id="cb87-19"><a href="lrwor.html#cb87-19" tabindex="-1"></a><span class="co"># print the summary to the console</span></span>
<span id="cb87-20"><a href="lrwor.html#cb87-20" tabindex="-1"></a>DistributionSummary</span>
<span id="cb87-21"><a href="lrwor.html#cb87-21" tabindex="-1"></a><span class="co">#&gt;               Average StandardDeviation quantile.10. quantile.25. quantile.40.</span></span>
<span id="cb87-22"><a href="lrwor.html#cb87-22" tabindex="-1"></a><span class="co">#&gt; quant_STR    19.64043          1.891812      17.3486     18.58236     19.26618</span></span>
<span id="cb87-23"><a href="lrwor.html#cb87-23" tabindex="-1"></a><span class="co">#&gt; quant_score 654.15655         19.053347     630.3950    640.05000    649.06999</span></span>
<span id="cb87-24"><a href="lrwor.html#cb87-24" tabindex="-1"></a><span class="co">#&gt;             quantile.50. quantile.60. quantile.75. quantile.90.</span></span>
<span id="cb87-25"><a href="lrwor.html#cb87-25" tabindex="-1"></a><span class="co">#&gt; quant_STR       19.72321      20.0783     20.87181     21.86741</span></span>
<span id="cb87-26"><a href="lrwor.html#cb87-26" tabindex="-1"></a><span class="co">#&gt; quant_score    654.45000     659.4000    666.66249    678.85999</span></span></code></pre></div>
<p>As for the sample data, we use <tt>plot()</tt>. This allows us to detect characteristics of our data, such as outliers which are harder to discover by looking at mere numbers. This time we add some additional arguments to the call of <tt>plot()</tt>.</p>
<p>The first argument in our call of <tt>plot()</tt>, <tt>score ~ STR</tt>, is again a formula that states variables on the y- and the x-axis. However, this time the two variables are not saved in separate vectors but are columns of <tt>CASchools</tt>. Therefore, <tt>R</tt> would not find them without the argument <tt>data</tt> being correctly specified. <tt>data</tt> must be in accordance with the name of the <tt>data.frame</tt> to which the variables belong to, in this case <tt>CASchools</tt>. Further arguments are used to change the appearance of the plot: <tt>main</tt> adds a title, <tt>xlab</tt> and <tt>ylab</tt> add custom labels to both axes.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="lrwor.html#cb88-1" tabindex="-1"></a><span class="fu">plot</span>(score <span class="sc">~</span> STR, </span>
<span id="cb88-2"><a href="lrwor.html#cb88-2" tabindex="-1"></a>     <span class="at">data =</span> CASchools,</span>
<span id="cb88-3"><a href="lrwor.html#cb88-3" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Scatterplot of Test Score and STR&quot;</span>, </span>
<span id="cb88-4"><a href="lrwor.html#cb88-4" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;STR (X)&quot;</span>,</span>
<span id="cb88-5"><a href="lrwor.html#cb88-5" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Test Score (Y)&quot;</span>)</span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-153-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>The plot (Figure 4.2 in the book) shows the scatterplot of all observations on the student-teacher ratio and test score. We see that the points are strongly scattered, and that the variables are negatively correlated. That is, we expect to observe lower test scores in bigger classes.</p>
<p>The function <tt>cor()</tt> (see <tt>?cor</tt> for further info) can be used to compute the correlation between two <em>numeric</em> vectors.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="lrwor.html#cb89-1" tabindex="-1"></a><span class="fu">cor</span>(CASchools<span class="sc">$</span>STR, CASchools<span class="sc">$</span>score)</span>
<span id="cb89-2"><a href="lrwor.html#cb89-2" tabindex="-1"></a><span class="co">#&gt; [1] -0.2263627</span></span></code></pre></div>
<p>As the scatterplot already suggests, the correlation is negative but rather weak.</p>
<p>The task we are currently facing is to find a line that best fits the data. We could opt for graphical inspection and correlation analysis and then select the best fitting line by eyeballing. However, this would be rather subjective: different observers would draw different regression lines. On this account, we are interested in techniques that are less arbitrary. Such a technique is given by ordinary least squares (OLS) estimation.</p>
<div id="the-ordinary-least-squares-estimator" class="section level3 unnumbered hasAnchor">
<h3>The Ordinary Least Squares Estimator<a href="lrwor.html#the-ordinary-least-squares-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The OLS estimator chooses the regression coefficients such that the estimated regression line is as “close” as possible to the observed data points. Here, closeness is measured by the sum of the squared mistakes made in predicting <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. Let <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> be some estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Then the sum of squared estimation mistakes can be expressed as</p>
<p><span class="math display">\[ \sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2. \]</span></p>
<p>The OLS estimator in the simple regression model is the pair of estimators for intercept and slope that minimizes the expression above. The derivation of the OLS estimators for both parameters are presented in Appendix 4.1 of the book. The results are summarized in Key Concept 4.2.</p>
<div id="KC4.2" class="keyconcept">
<h3 class="right">
Key Concept 4.2
</h3>
<h3 class="left">
The OLS Estimator, Predicted Values, and Residuals
</h3>
<p>
<p>The OLS estimators of the slope <span class="math inline">\(\beta_1\)</span> and the intercept <span class="math inline">\(\beta_0\)</span> in the simple linear regression model are
<span class="math display">\[\begin{align}
  \hat\beta_1 &amp; = \frac{ \sum_{i = 1}^n (X_i - \overline{X})(Y_i - \overline{Y}) } { \sum_{i=1}^n (X_i - \overline{X})^2},  \\
  \\
  \hat\beta_0 &amp; =  \overline{Y} - \hat\beta_1 \overline{X}.
\end{align}\]</span>
The OLS predicted values <span class="math inline">\(\widehat{Y}_i\)</span> and residuals <span class="math inline">\(\hat{u}_i\)</span> are
<span class="math display">\[\begin{align}
  \widehat{Y}_i &amp; =  \hat\beta_0 + \hat\beta_1 X_i,\\
  \\
  \hat{u}_i &amp; =  Y_i - \widehat{Y}_i.
\end{align}\]</span></p>
The estimated intercept <span class="math inline">\(\hat{\beta}_0\)</span>, the slope parameter <span class="math inline">\(\hat{\beta}_1\)</span> and the residuals <span class="math inline">\(\left(\hat{u}_i\right)\)</span> are computed from a sample of <span class="math inline">\(n\)</span> observations of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span>,<span class="math inline">\(i\)</span>=<span class="math inline">\(1\)</span>, <span class="math inline">\(...\)</span>,<span class="math inline">\(n\)</span>. These are <em>estimates</em> of the unknown population intercept <span class="math inline">\(\left(\beta_0 \right)\)</span>, slope <span class="math inline">\(\left(\beta_1\right)\)</span>, and error term <span class="math inline">\((u_i)\)</span>.
</p>
<p>The formulas presented above may not be very intuitive at first glance. The following interactive application aims to help you understand the mechanics of OLS. You can add observations by clicking into the coordinate system where the data are represented by points. Once two or more observations are available, the application computes a regression line using OLS and some statistics which are displayed in the right panel. The results are updated as you add further observations to the left panel. A double-click resets the application, i.e., all data are removed.</p>
<iframe height="410" width="900" frameborder="0" scrolling="no" src="SimpleRegression.html">
</iframe>
</div>
<p>There are many possible ways to compute <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> in <tt>R</tt>. For example, we could implement the formulas presented in Key Concept 4.2 with two of <tt>R</tt>’s most basic functions: <tt>mean()</tt> and <tt>sum()</tt>. Before doing so we <em>attach</em> the <tt>CASchools</tt> dataset.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="lrwor.html#cb90-1" tabindex="-1"></a><span class="fu">attach</span>(CASchools) <span class="co"># allows to use the variables contained in CASchools directly</span></span>
<span id="cb90-2"><a href="lrwor.html#cb90-2" tabindex="-1"></a></span>
<span id="cb90-3"><a href="lrwor.html#cb90-3" tabindex="-1"></a><span class="co"># compute beta_1_hat</span></span>
<span id="cb90-4"><a href="lrwor.html#cb90-4" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> <span class="fu">sum</span>((STR <span class="sc">-</span> <span class="fu">mean</span>(STR)) <span class="sc">*</span> (score <span class="sc">-</span> <span class="fu">mean</span>(score))) <span class="sc">/</span> <span class="fu">sum</span>((STR <span class="sc">-</span> <span class="fu">mean</span>(STR))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb90-5"><a href="lrwor.html#cb90-5" tabindex="-1"></a></span>
<span id="cb90-6"><a href="lrwor.html#cb90-6" tabindex="-1"></a><span class="co"># compute beta_0_hat</span></span>
<span id="cb90-7"><a href="lrwor.html#cb90-7" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(score) <span class="sc">-</span> beta_1 <span class="sc">*</span> <span class="fu">mean</span>(STR)</span>
<span id="cb90-8"><a href="lrwor.html#cb90-8" tabindex="-1"></a></span>
<span id="cb90-9"><a href="lrwor.html#cb90-9" tabindex="-1"></a><span class="co"># print the results to the console</span></span>
<span id="cb90-10"><a href="lrwor.html#cb90-10" tabindex="-1"></a>beta_1</span>
<span id="cb90-11"><a href="lrwor.html#cb90-11" tabindex="-1"></a><span class="co">#&gt; [1] -2.279808</span></span>
<span id="cb90-12"><a href="lrwor.html#cb90-12" tabindex="-1"></a>beta_0</span>
<span id="cb90-13"><a href="lrwor.html#cb90-13" tabindex="-1"></a><span class="co">#&gt; [1] 698.9329</span></span></code></pre></div>

<div class="rmdknit">
<p>Calling <tt>attach(CASchools)</tt> enables us to address a variable contained in <tt>CASchools</tt> by its name: it is no longer necessary to use the <tt>$</tt> operator in conjunction with the dataset: <tt>R</tt> may evaluate the variable name directly.</p>
<tt>R</tt> uses the object in the user environment if this object shares the name of variable contained in an attached database. However, it is a better practice to always use distinctive names in order to avoid such (seeming) ambivalences!
</div>
<p><br></p>
<p><strong>Notice that we address variables contained in the attached dataset <tt>CASchools</tt> directly for the rest of this chapter!</strong></p>
<p>Of course, there are even more manual ways to perform these tasks. With OLS being one of the most widely-used estimation techniques, <tt>R</tt> of course already contains a built-in function named <tt>lm()</tt> (<strong>l</strong>inear <strong>m</strong>odel) which can be used to carry out regression analysis.</p>
<p>The first argument of the function to be specified is, similar to <tt>plot()</tt>, the regression formula with the basic syntax <tt>y ~ x</tt> where <tt>y</tt> is the dependent variable and <tt>x</tt> the explanatory variable. The argument <tt>data</tt> determines the data set to be used in the regression. We now revisit the example from the book where the relationship between the test scores and the class sizes is analyzed. The following code uses <tt>lm()</tt> to replicate the results presented in figure 4.3 of the book.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="lrwor.html#cb91-1" tabindex="-1"></a><span class="co"># estimate the model and assign the result to linear_model</span></span>
<span id="cb91-2"><a href="lrwor.html#cb91-2" tabindex="-1"></a>linear_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> STR, <span class="at">data =</span> CASchools)</span>
<span id="cb91-3"><a href="lrwor.html#cb91-3" tabindex="-1"></a></span>
<span id="cb91-4"><a href="lrwor.html#cb91-4" tabindex="-1"></a><span class="co"># print the standard output of the estimated lm object to the console </span></span>
<span id="cb91-5"><a href="lrwor.html#cb91-5" tabindex="-1"></a>linear_model</span>
<span id="cb91-6"><a href="lrwor.html#cb91-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb91-7"><a href="lrwor.html#cb91-7" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb91-8"><a href="lrwor.html#cb91-8" tabindex="-1"></a><span class="co">#&gt; lm(formula = score ~ STR, data = CASchools)</span></span>
<span id="cb91-9"><a href="lrwor.html#cb91-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb91-10"><a href="lrwor.html#cb91-10" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb91-11"><a href="lrwor.html#cb91-11" tabindex="-1"></a><span class="co">#&gt; (Intercept)          STR  </span></span>
<span id="cb91-12"><a href="lrwor.html#cb91-12" tabindex="-1"></a><span class="co">#&gt;      698.93        -2.28</span></span></code></pre></div>
<p>Let us add the estimated regression line to the plot. This time we also enlarge the ranges of both axes by setting the arguments <tt>xlim</tt> and <tt>ylim</tt>.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="lrwor.html#cb92-1" tabindex="-1"></a><span class="co"># plot the data</span></span>
<span id="cb92-2"><a href="lrwor.html#cb92-2" tabindex="-1"></a><span class="fu">plot</span>(score <span class="sc">~</span> STR, </span>
<span id="cb92-3"><a href="lrwor.html#cb92-3" tabindex="-1"></a>     <span class="at">data =</span> CASchools,</span>
<span id="cb92-4"><a href="lrwor.html#cb92-4" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Scatterplot of Test Score and STR&quot;</span>, </span>
<span id="cb92-5"><a href="lrwor.html#cb92-5" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;STR (X)&quot;</span>,</span>
<span id="cb92-6"><a href="lrwor.html#cb92-6" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Test Score (Y)&quot;</span>,</span>
<span id="cb92-7"><a href="lrwor.html#cb92-7" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">30</span>),</span>
<span id="cb92-8"><a href="lrwor.html#cb92-8" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">600</span>, <span class="dv">720</span>))</span>
<span id="cb92-9"><a href="lrwor.html#cb92-9" tabindex="-1"></a></span>
<span id="cb92-10"><a href="lrwor.html#cb92-10" tabindex="-1"></a><span class="co"># add the regression line</span></span>
<span id="cb92-11"><a href="lrwor.html#cb92-11" tabindex="-1"></a><span class="fu">abline</span>(linear_model) </span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-159-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Did you notice that this time, we did not pass the intercept and slope parameters to <tt>abline</tt>? If you call <tt>abline()</tt> on an object of class <tt>lm</tt> which only contains a single regressor, <tt>R</tt> draws the regression line automatically!</p>
</div>
</div>
<div id="measures-of-fit" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Measures of Fit<a href="lrwor.html#measures-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After fitting a linear regression model, a natural question is how well the model describes the data. Visually, this amounts to assessing whether the observations are tightly clustered around the regression line. Both the <em>coefficient of determination</em> and the <em>standard error of the regression</em> measure how well the OLS Regression line fits the data.</p>
<div id="the-coefficient-of-determination" class="section level3 unnumbered hasAnchor">
<h3>The Coefficient of Determination<a href="lrwor.html#the-coefficient-of-determination" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(R^2\)</span>, the <em>coefficient of determination</em>, is the fraction of the sample variance of <span class="math inline">\(Y_i\)</span> that is explained by <span class="math inline">\(X_i\)</span>. Mathematically, the <span class="math inline">\(R^2\)</span> can be written as the ratio of the explained sum of squares to the total sum of squares. The <em>explained sum of squares</em> (<span class="math inline">\(ESS\)</span>) is the sum of squared deviations of the predicted values <span class="math inline">\(\hat{Y_i}\)</span>, from the average of the <span class="math inline">\(Y_i\)</span>. The <em>total sum of squares</em> (<span class="math inline">\(TSS\)</span>) is the sum of squared deviations of the <span class="math inline">\(Y_i\)</span> from their average. Thus we have</p>
<p><span class="math display">\[\begin{align}
  ESS &amp; =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2,   \\
  TSS &amp; =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2,   \\
  R^2 &amp; = \frac{ESS}{TSS}.
\end{align}\]</span></p>
<p>Since <span class="math inline">\(TSS = ESS + SSR\)</span> we can also write</p>
<p><span class="math display">\[ R^2 = 1- \frac{SSR}{TSS}, \]</span></p>
<p>where <span class="math inline">\(SSR\)</span> is the sum of squared residuals, a measure for the errors made when predicting <span class="math inline">\(Y\)</span> by <span class="math inline">\(X\)</span>. The <span class="math inline">\(SSR\)</span> is defined as</p>
<p><span class="math display">\[ SSR = \sum_{i=1}^n \hat{u}_i^2. \]</span></p>
<p><span class="math inline">\(R^2\)</span> lies between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. It is easy to see that a perfect fit, i.e., no errors made when fitting the regression line, implies <span class="math inline">\(R^2 = 1\)</span> then we have <span class="math inline">\(SSR=0\)</span>. On the contrary, if our estimated regression line does not explain any variation in the <span class="math inline">\(Y_i\)</span>, we have <span class="math inline">\(ESS=0\)</span> and consequently <span class="math inline">\(R^2=0\)</span>.</p>
</div>
<div id="the-standard-error-of-the-regression" class="section level3 unnumbered hasAnchor">
<h3>The Standard Error of the Regression<a href="lrwor.html#the-standard-error-of-the-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <em>Standard Error of the Regression</em> (<span class="math inline">\(SER\)</span>) is an estimator of the standard deviation of the residuals <span class="math inline">\(\hat{u}_i\)</span>. As such it measures the magnitude of a typical deviation from the regression line, i.e., the magnitude of a typical residual.</p>
<p><span class="math display">\[ SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2} \ \ \ \text{where} \ \ \ s_{\hat{u} }^2 = \frac{1}{n-2} \sum_{i = 1}^n \hat{u}^2_i = \frac{SSR}{n - 2} \]</span></p>
<p>Remember that the <span class="math inline">\(u_i\)</span> are <em>unobserved</em>. This is why we use their estimated counterparts, the residuals <span class="math inline">\(\hat{u}_i\)</span>, instead. See Chapter 4.3 of the book for a more detailed comment on the <span class="math inline">\(SER\)</span>.</p>
</div>
<div id="application-to-the-test-score-data" class="section level3 unnumbered hasAnchor">
<h3>Application to the Test Score Data<a href="lrwor.html#application-to-the-test-score-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Both measures of fit can be obtained by using the function <tt>summary()</tt> with an <tt>lm</tt> object provided as the only argument. While the function <tt>lm()</tt> only prints out the estimated coefficients to the console, <tt>summary()</tt> provides additional predefined information such as the regression’s <span class="math inline">\(R^2\)</span> and the <span class="math inline">\(SER\)</span>.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="lrwor.html#cb93-1" tabindex="-1"></a>mod_summary <span class="ot">&lt;-</span> <span class="fu">summary</span>(linear_model)</span>
<span id="cb93-2"><a href="lrwor.html#cb93-2" tabindex="-1"></a>mod_summary</span>
<span id="cb93-3"><a href="lrwor.html#cb93-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb93-4"><a href="lrwor.html#cb93-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb93-5"><a href="lrwor.html#cb93-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = score ~ STR, data = CASchools)</span></span>
<span id="cb93-6"><a href="lrwor.html#cb93-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb93-7"><a href="lrwor.html#cb93-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb93-8"><a href="lrwor.html#cb93-8" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb93-9"><a href="lrwor.html#cb93-9" tabindex="-1"></a><span class="co">#&gt; -47.727 -14.251   0.483  12.822  48.540 </span></span>
<span id="cb93-10"><a href="lrwor.html#cb93-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb93-11"><a href="lrwor.html#cb93-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb93-12"><a href="lrwor.html#cb93-12" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb93-13"><a href="lrwor.html#cb93-13" tabindex="-1"></a><span class="co">#&gt; (Intercept) 698.9329     9.4675  73.825  &lt; 2e-16 ***</span></span>
<span id="cb93-14"><a href="lrwor.html#cb93-14" tabindex="-1"></a><span class="co">#&gt; STR          -2.2798     0.4798  -4.751 2.78e-06 ***</span></span>
<span id="cb93-15"><a href="lrwor.html#cb93-15" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb93-16"><a href="lrwor.html#cb93-16" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb93-17"><a href="lrwor.html#cb93-17" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb93-18"><a href="lrwor.html#cb93-18" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 18.58 on 418 degrees of freedom</span></span>
<span id="cb93-19"><a href="lrwor.html#cb93-19" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.05124,    Adjusted R-squared:  0.04897 </span></span>
<span id="cb93-20"><a href="lrwor.html#cb93-20" tabindex="-1"></a><span class="co">#&gt; F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06</span></span></code></pre></div>
<p>The <span class="math inline">\(R^2\)</span> in the output is called <em>Multiple R-squared</em> and has a value of <span class="math inline">\(0.051\)</span>. Hence, <span class="math inline">\(5.1 \%\)</span> of the variance of the dependent variable <span class="math inline">\(score\)</span> is explained by the explanatory variable <span class="math inline">\(STR\)</span>. That is, the regression explains little of the variance in <span class="math inline">\(score\)</span>, and much of the variation in test scores remains unexplained (cf. Figure 4.3 of the book).</p>
<p>The <span class="math inline">\(SER\)</span> is called <em>Residual standard error</em> and equals <span class="math inline">\(18.58\)</span>. The unit of the <span class="math inline">\(SER\)</span> is the same as the unit of the dependent variable. That is, on average the deviation of the actual achieved test score and the regression line is <span class="math inline">\(18.58\)</span> points.</p>
<p>Now, let us check whether <tt>summary()</tt> uses the same definitions for <span class="math inline">\(R^2\)</span> and <span class="math inline">\(SER\)</span> as we do when computing them manually.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="lrwor.html#cb94-1" tabindex="-1"></a><span class="co"># compute R^2 manually</span></span>
<span id="cb94-2"><a href="lrwor.html#cb94-2" tabindex="-1"></a>SSR <span class="ot">&lt;-</span> <span class="fu">sum</span>(mod_summary<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb94-3"><a href="lrwor.html#cb94-3" tabindex="-1"></a>TSS <span class="ot">&lt;-</span> <span class="fu">sum</span>((score <span class="sc">-</span> <span class="fu">mean</span>(score))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb94-4"><a href="lrwor.html#cb94-4" tabindex="-1"></a>R2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> SSR<span class="sc">/</span>TSS</span>
<span id="cb94-5"><a href="lrwor.html#cb94-5" tabindex="-1"></a></span>
<span id="cb94-6"><a href="lrwor.html#cb94-6" tabindex="-1"></a><span class="co"># print the value to the console</span></span>
<span id="cb94-7"><a href="lrwor.html#cb94-7" tabindex="-1"></a>R2</span>
<span id="cb94-8"><a href="lrwor.html#cb94-8" tabindex="-1"></a><span class="co">#&gt; [1] 0.05124009</span></span>
<span id="cb94-9"><a href="lrwor.html#cb94-9" tabindex="-1"></a></span>
<span id="cb94-10"><a href="lrwor.html#cb94-10" tabindex="-1"></a><span class="co"># compute SER manually</span></span>
<span id="cb94-11"><a href="lrwor.html#cb94-11" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(CASchools)</span>
<span id="cb94-12"><a href="lrwor.html#cb94-12" tabindex="-1"></a>SER <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(SSR <span class="sc">/</span> (n<span class="dv">-2</span>))</span>
<span id="cb94-13"><a href="lrwor.html#cb94-13" tabindex="-1"></a></span>
<span id="cb94-14"><a href="lrwor.html#cb94-14" tabindex="-1"></a><span class="co"># print the value to the console</span></span>
<span id="cb94-15"><a href="lrwor.html#cb94-15" tabindex="-1"></a>SER</span>
<span id="cb94-16"><a href="lrwor.html#cb94-16" tabindex="-1"></a><span class="co">#&gt; [1] 18.58097</span></span></code></pre></div>
<p>We find that the results coincide. Note that the values provided by <tt>summary()</tt> are rounded to two decimal places.</p>
</div>
</div>
<div id="tlsa" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> The Least Squares Assumptions<a href="lrwor.html#tlsa" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>OLS performs well under a quite broad variety of different circumstances. However, there are some assumptions which need to be satisfied in order to ensure that the estimates are normally distributed in large samples (we discuss this in Chapter <a href="lrwor.html#tsdotoe">4.5</a>).</p>
<div id="KC4.3" class="keyconcept">
<h3 class="right">
Key Concept 4.3
</h3>
<h3 class="left">
The Least Squares Assumptions
</h3>
<p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i \text{, } i = 1,\dots,n\]</span>
where</p>
<ol style="list-style-type: decimal">
<li>The error term <span class="math inline">\(u_i\)</span> has conditional mean zero given <span class="math inline">\(X_i\)</span>: <span class="math inline">\(E(u_i|X_i) = 0\)</span>.</li>
<li><span class="math inline">\((X_i,Y_i), i = 1,\dots,n\)</span> are independent and identically distributed (i.i.d.) draws from their joint distribution.</li>
<li>Large outliers are unlikely: <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> have nonzero finite fourth moments.</li>
</ol>
</p>
</div>
<div id="assumption-1-the-error-term-has-conditional-mean-of-zero" class="section level3 unnumbered hasAnchor">
<h3>Assumption 1: The Error Term has Conditional Mean of Zero<a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This means that no matter which value we choose for <span class="math inline">\(X\)</span>, the error term <span class="math inline">\(u\)</span> must not show any systematic pattern and must have a mean of <span class="math inline">\(0\)</span>.
Consider the case that, unconditionally, <span class="math inline">\(E(u) = 0\)</span>, but for low and high values of <span class="math inline">\(X\)</span>, the error term tends to be positive and for midrange values of
<span class="math inline">\(X\)</span> the error tends to be negative. We can use R to construct such an example. To do so we generate our own data using <tt>R</tt>’s built-in random number generators.</p>
<p>We will use the following functions:</p>
<ul>
<li><tt>runif()</tt> - generates uniformly distributed random numbers.</li>
<li><tt>rnorm()</tt> - generates normally distributed random numbers.</li>
<li><tt>predict()</tt> - does predictions based on the results of model fitting functions like <tt>lm()</tt>.</li>
<li><tt>lines()</tt> - adds line segments to an existing plot.</li>
</ul>
<p>We start by creating a vector containing values that are uniformly distributed on the interval <span class="math inline">\([-5,5]\)</span>. This can be done with the function <tt>runif()</tt>. We also need to simulate the error term. For this we generate normally distributed random numbers with a mean of <span class="math inline">\(0\)</span> and a variance of <span class="math inline">\(1\)</span> using <tt>rnorm()</tt>. The <span class="math inline">\(Y\)</span> values are obtained as a quadratic function of the <span class="math inline">\(X\)</span> values and the error.</p>
<p>After generating the data we estimate both a simple regression model and a quadratic model that also includes the regressor <span class="math inline">\(X^2\)</span> (this is a multiple regression model, see Chapter <a href="rmwmr.html#rmwmr">6</a>). Finally, we plot the simulated data and add the estimated regression line of a simple regression model as well as the predictions made with a quadratic model to compare the fit graphically.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="lrwor.html#cb95-1" tabindex="-1"></a><span class="co"># set a seed to make the results reproducible</span></span>
<span id="cb95-2"><a href="lrwor.html#cb95-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">321</span>)</span>
<span id="cb95-3"><a href="lrwor.html#cb95-3" tabindex="-1"></a></span>
<span id="cb95-4"><a href="lrwor.html#cb95-4" tabindex="-1"></a><span class="co"># simulate the data </span></span>
<span id="cb95-5"><a href="lrwor.html#cb95-5" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">50</span>, <span class="at">min =</span> <span class="sc">-</span><span class="dv">5</span>, <span class="at">max =</span> <span class="dv">5</span>)</span>
<span id="cb95-6"><a href="lrwor.html#cb95-6" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, <span class="at">sd =</span> <span class="dv">1</span>)  </span>
<span id="cb95-7"><a href="lrwor.html#cb95-7" tabindex="-1"></a></span>
<span id="cb95-8"><a href="lrwor.html#cb95-8" tabindex="-1"></a><span class="co"># the true relation  </span></span>
<span id="cb95-9"><a href="lrwor.html#cb95-9" tabindex="-1"></a>Y <span class="ot">&lt;-</span> X<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> X <span class="sc">+</span> u                </span>
<span id="cb95-10"><a href="lrwor.html#cb95-10" tabindex="-1"></a></span>
<span id="cb95-11"><a href="lrwor.html#cb95-11" tabindex="-1"></a><span class="co"># estimate a simple regression model </span></span>
<span id="cb95-12"><a href="lrwor.html#cb95-12" tabindex="-1"></a>mod_simple <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X)</span>
<span id="cb95-13"><a href="lrwor.html#cb95-13" tabindex="-1"></a></span>
<span id="cb95-14"><a href="lrwor.html#cb95-14" tabindex="-1"></a><span class="co"># estimate a quadratic regression model</span></span>
<span id="cb95-15"><a href="lrwor.html#cb95-15" tabindex="-1"></a>mod_quadratic <span class="ot">&lt;-</span> <span class="fu">lm</span>( Y <span class="sc">~</span> X <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">2</span>)) </span>
<span id="cb95-16"><a href="lrwor.html#cb95-16" tabindex="-1"></a></span>
<span id="cb95-17"><a href="lrwor.html#cb95-17" tabindex="-1"></a><span class="co"># predict using a quadratic model </span></span>
<span id="cb95-18"><a href="lrwor.html#cb95-18" tabindex="-1"></a>prediction <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_quadratic, <span class="fu">data.frame</span>(<span class="at">X =</span> <span class="fu">sort</span>(X)))</span>
<span id="cb95-19"><a href="lrwor.html#cb95-19" tabindex="-1"></a></span>
<span id="cb95-20"><a href="lrwor.html#cb95-20" tabindex="-1"></a><span class="co"># plot the results</span></span>
<span id="cb95-21"><a href="lrwor.html#cb95-21" tabindex="-1"></a><span class="fu">plot</span>( Y <span class="sc">~</span> X, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb95-22"><a href="lrwor.html#cb95-22" tabindex="-1"></a><span class="fu">abline</span>( mod_simple, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb95-23"><a href="lrwor.html#cb95-23" tabindex="-1"></a><span class="co">#red line = incorrect linear regression (this violates the first OLS assumption)</span></span>
<span id="cb95-24"><a href="lrwor.html#cb95-24" tabindex="-1"></a><span class="fu">lines</span>( <span class="fu">sort</span>(X), prediction,<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb95-25"><a href="lrwor.html#cb95-25" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, </span>
<span id="cb95-26"><a href="lrwor.html#cb95-26" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Simple Regression Model&quot;</span>, </span>
<span id="cb95-27"><a href="lrwor.html#cb95-27" tabindex="-1"></a>                  <span class="st">&quot;Quadratic Model&quot;</span>),</span>
<span id="cb95-28"><a href="lrwor.html#cb95-28" tabindex="-1"></a>       <span class="at">cex =</span> <span class="dv">1</span>,</span>
<span id="cb95-29"><a href="lrwor.html#cb95-29" tabindex="-1"></a>       <span class="at">lty =</span> <span class="dv">1</span>,</span>
<span id="cb95-30"><a href="lrwor.html#cb95-30" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>))</span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-164-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>The plot above shows what is meant by <span class="math inline">\(E(u_i|X_i) = 0\)</span> and why it does not hold for the linear model:</p>
<p>Using the quadratic model (represented by the black curve) we see that there are no systematic deviations of the observation from the predicted relation. It is credible that the assumption is not violated when such a model is employed. However, using a simple linear regression model we see that the assumption is probably violated as <span class="math inline">\(E(u_i|X_i)\)</span> varies with the <span class="math inline">\(X_i\)</span>.</p>
</div>
<div id="assumption-2-independently-and-identically-distributed-data" class="section level3 unnumbered hasAnchor">
<h3>Assumption 2: Independently and Identically Distributed Data<a href="lrwor.html#assumption-2-independently-and-identically-distributed-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Most sampling schemes used when collecting data from populations produce i.i.d.-samples. For example, we can use <tt>R</tt>’s random number generator to randomly select student IDs from a university’s enrollment list and record age <span class="math inline">\(X\)</span> and earnings <span class="math inline">\(Y\)</span> of the corresponding students. This is a typical example of simple random sampling and ensures that all the <span class="math inline">\((X_i, Y_i)\)</span> are drawn randomly from the same population.</p>
<p>A prominent example where the i.i.d. assumption is not fulfilled is time series data where we have observations on the same unit over time. For example, take <span class="math inline">\(X\)</span> as the number of workers in a production company over time. Due to business transformations, the company cuts jobs periodically by a specific share but there are also some non-deterministic influences that relate to economics, politics etc. Using <tt>R</tt> we can easily simulate such a process and plot it.</p>
<p>We start the series with a total of 5000 workers and simulate the reduction of employment with an autoregressive process that exhibits a downward movement in the long-run and has normally distributed errors:<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p><span class="math display">\[ employment_t = -50 + 0.98 \cdot employment_{t-1} + u_t. \]</span></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="lrwor.html#cb96-1" tabindex="-1"></a><span class="co"># set seed</span></span>
<span id="cb96-2"><a href="lrwor.html#cb96-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb96-3"><a href="lrwor.html#cb96-3" tabindex="-1"></a></span>
<span id="cb96-4"><a href="lrwor.html#cb96-4" tabindex="-1"></a><span class="co"># generate a date vector</span></span>
<span id="cb96-5"><a href="lrwor.html#cb96-5" tabindex="-1"></a>Date <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">as.Date</span>(<span class="st">&quot;1951/1/1&quot;</span>), <span class="fu">as.Date</span>(<span class="st">&quot;2000/1/1&quot;</span>), <span class="st">&quot;years&quot;</span>)</span>
<span id="cb96-6"><a href="lrwor.html#cb96-6" tabindex="-1"></a></span>
<span id="cb96-7"><a href="lrwor.html#cb96-7" tabindex="-1"></a><span class="co"># initialize the employment vector</span></span>
<span id="cb96-8"><a href="lrwor.html#cb96-8" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5000</span>, <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="fu">length</span>(Date)<span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb96-9"><a href="lrwor.html#cb96-9" tabindex="-1"></a></span>
<span id="cb96-10"><a href="lrwor.html#cb96-10" tabindex="-1"></a><span class="co"># generate time series observations with random influences</span></span>
<span id="cb96-11"><a href="lrwor.html#cb96-11" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(Date)) {</span>
<span id="cb96-12"><a href="lrwor.html#cb96-12" tabindex="-1"></a>  </span>
<span id="cb96-13"><a href="lrwor.html#cb96-13" tabindex="-1"></a>    X[t] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">50</span> <span class="sc">+</span> <span class="fl">0.98</span> <span class="sc">*</span> X[t<span class="dv">-1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">200</span>)</span>
<span id="cb96-14"><a href="lrwor.html#cb96-14" tabindex="-1"></a>    </span>
<span id="cb96-15"><a href="lrwor.html#cb96-15" tabindex="-1"></a>}</span>
<span id="cb96-16"><a href="lrwor.html#cb96-16" tabindex="-1"></a></span>
<span id="cb96-17"><a href="lrwor.html#cb96-17" tabindex="-1"></a><span class="co">#plot the results</span></span>
<span id="cb96-18"><a href="lrwor.html#cb96-18" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> Date, </span>
<span id="cb96-19"><a href="lrwor.html#cb96-19" tabindex="-1"></a>     <span class="at">y =</span> X, </span>
<span id="cb96-20"><a href="lrwor.html#cb96-20" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb96-21"><a href="lrwor.html#cb96-21" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>, </span>
<span id="cb96-22"><a href="lrwor.html#cb96-22" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Workers&quot;</span>, </span>
<span id="cb96-23"><a href="lrwor.html#cb96-23" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Time&quot;</span>,</span>
<span id="cb96-24"><a href="lrwor.html#cb96-24" tabindex="-1"></a>     <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-165-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>It is evident that the observations on the number of employees cannot be independent in this example: the level of today’s employment is correlated with tomorrow’s employment level. Thus, the i.i.d. assumption is violated.</p>
</div>
<div id="assumption-3-large-outliers-are-unlikely" class="section level3 unnumbered hasAnchor">
<h3>Assumption 3: Large Outliers are Unlikely<a href="lrwor.html#assumption-3-large-outliers-are-unlikely" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is easy to come up with situations where extreme observations, i.e., observations that deviate considerably from the usual range of the data, may occur. Such observations are called outliers. Technically speaking, assumption 3 requires that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a finite kurtosis.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>Common cases where we want to exclude or (if possible) correct such outliers is when they are apparently typos, conversion errors or measurement errors. Even if it seems like extreme observations have been recorded correctly, it is advisable to exclude them before estimating a model since OLS suffers from <em>sensitivity to outliers</em>.</p>
<p>What does this mean? One can show that extreme observations receive heavy weighting in the estimation of the unknown regression coefficients when using OLS. Therefore, outliers can lead to strongly distorted estimates of regression coefficients. To get a better impression of this issue, consider the following application where we have placed some sample data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> which are highly correlated. The relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> seems to be explained pretty well by the plotted regression line: all of the white data points lie close to the red regression line and we have <span class="math inline">\(R^2=0.92\)</span>.</p>
<p>Now go ahead and add a further observation at, say, <span class="math inline">\((18.2)\)</span>. This observation clearly is an outlier. The result is quite striking: the estimated regression line differs greatly from the one we adjudged to fit the data well. The slope is heavily downward biased and <span class="math inline">\(R^2\)</span> decreased to a mere <span class="math inline">\(29\%\)</span>! <br>
Double-click inside the coordinate system to reset the app. Feel free to experiment. Choose different coordinates for the outlier or add additional ones.</p>
<iframe height="410" width="900" frameborder="0" scrolling="no" src="Outlier.html">
</iframe>
<p>The following code roughly reproduces what is shown in figure 4.5 in the book. As done above we use sample data generated using <tt>R</tt>’s random number functions <tt>rnorm()</tt> and <tt>runif()</tt>. We estimate two simple regression models, one based on the original data set and another using a modified set where one observation is changed to be an outlier and then plot the results. In order to understand the complete code you should be familiar with the function <tt>sort()</tt> which sorts the entries of a numeric vector in ascending order.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="lrwor.html#cb97-1" tabindex="-1"></a><span class="co"># set seed</span></span>
<span id="cb97-2"><a href="lrwor.html#cb97-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb97-3"><a href="lrwor.html#cb97-3" tabindex="-1"></a></span>
<span id="cb97-4"><a href="lrwor.html#cb97-4" tabindex="-1"></a><span class="co"># generate the data</span></span>
<span id="cb97-5"><a href="lrwor.html#cb97-5" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(<span class="dv">10</span>, <span class="at">min =</span> <span class="dv">30</span>, <span class="at">max =</span> <span class="dv">70</span>))</span>
<span id="cb97-6"><a href="lrwor.html#cb97-6" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10</span> , <span class="at">mean =</span> <span class="dv">200</span>, <span class="at">sd =</span> <span class="dv">50</span>)</span>
<span id="cb97-7"><a href="lrwor.html#cb97-7" tabindex="-1"></a>Y[<span class="dv">9</span>] <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb97-8"><a href="lrwor.html#cb97-8" tabindex="-1"></a></span>
<span id="cb97-9"><a href="lrwor.html#cb97-9" tabindex="-1"></a><span class="co"># fit model with outlier</span></span>
<span id="cb97-10"><a href="lrwor.html#cb97-10" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X)</span>
<span id="cb97-11"><a href="lrwor.html#cb97-11" tabindex="-1"></a></span>
<span id="cb97-12"><a href="lrwor.html#cb97-12" tabindex="-1"></a><span class="co"># fit model without outlier</span></span>
<span id="cb97-13"><a href="lrwor.html#cb97-13" tabindex="-1"></a>fitWithoutOutlier <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y[<span class="sc">-</span><span class="dv">9</span>] <span class="sc">~</span> X[<span class="sc">-</span><span class="dv">9</span>])</span>
<span id="cb97-14"><a href="lrwor.html#cb97-14" tabindex="-1"></a></span>
<span id="cb97-15"><a href="lrwor.html#cb97-15" tabindex="-1"></a><span class="co"># plot the results</span></span>
<span id="cb97-16"><a href="lrwor.html#cb97-16" tabindex="-1"></a><span class="fu">plot</span>(Y <span class="sc">~</span> X,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb97-17"><a href="lrwor.html#cb97-17" tabindex="-1"></a><span class="fu">abline</span>(fit,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>)</span>
<span id="cb97-18"><a href="lrwor.html#cb97-18" tabindex="-1"></a><span class="fu">abline</span>(fitWithoutOutlier, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb97-19"><a href="lrwor.html#cb97-19" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, </span>
<span id="cb97-20"><a href="lrwor.html#cb97-20" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Model with Outlier&quot;</span>, </span>
<span id="cb97-21"><a href="lrwor.html#cb97-21" tabindex="-1"></a>                  <span class="st">&quot;Model without Outlier&quot;</span>),</span>
<span id="cb97-22"><a href="lrwor.html#cb97-22" tabindex="-1"></a>       <span class="at">cex =</span> <span class="dv">1</span>,</span>
<span id="cb97-23"><a href="lrwor.html#cb97-23" tabindex="-1"></a>       <span class="at">lty =</span> <span class="dv">1</span>,</span>
<span id="cb97-24"><a href="lrwor.html#cb97-24" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>))</span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-166-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="tsdotoe" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> The Sampling Distribution of the OLS Estimator<a href="lrwor.html#tsdotoe" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Because <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are computed from a sample, the estimators themselves are random variables with a probability distribution — the so-called sampling distribution of the estimators — which describes the values they could take on over different samples. Although the sampling distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> can be complicated when the sample size is small and generally changes with the number of observations, <span class="math inline">\(n\)</span>, it is possible, provided the assumptions discussed in the book are valid, to make certain statements about it that hold for all <span class="math inline">\(n\)</span>. In particular
<span class="math display">\[ E(\hat{\beta}_0) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta}_1) = \beta_1,\]</span>
that is, <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are unbiased estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the true parameters. If the sample is sufficiently large, by the central limit theorem the <em>joint</em> sampling distribution of the estimators is well approximated by the bivariate normal distribution (<a href="#mjx-eqn-2.1">2.1</a>). This implies that the marginal distributions are also normal in large samples. Core facts on the large-sample distributions of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are presented in Key Concept 4.4.</p>
<div id="KC4.4" class="keyconcept">
<h3 class="right">
Key Concept 4.4
</h3>
<h3 class="left">
Large Sample Distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>
</h3>
<p>
<p>If the least squares assumptions in Key Concept 4.3 hold, then in large samples <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> have a joint normal sampling distribution. The large sample normal distribution of <span class="math inline">\(\hat\beta_1\)</span> is <span class="math inline">\(\mathcal{N}(\beta_1, \sigma^2_{\hat\beta_1})\)</span>, where the variance of the distribution, <span class="math inline">\(\sigma^2_{\hat\beta_1}\)</span>, is</p>
<p><span class="math display" id="eq:olsvar1">\[\begin{align}
\sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2}. \tag{4.1}
\end{align}\]</span></p>
<p>The large sample normal distribution of <span class="math inline">\(\hat\beta_0\)</span> is <span class="math inline">\(\mathcal{N}(\beta_0, \sigma^2_{\hat\beta_0})\)</span> with</p>
<p><span class="math display" id="eq:olsvar2">\[\begin{align}
\sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. \tag{4.2}
\end{align}\]</span></p>
<p>The interactive simulation below continuously generates random samples <span class="math inline">\((X_i,Y_i)\)</span> of <span class="math inline">\(200\)</span> observations where <span class="math inline">\(E(Y\vert X) = 100 + 3X\)</span>, estimates a simple regression model, stores the estimate of the slope <span class="math inline">\(\beta_1\)</span> and visualizes the distribution of the <span class="math inline">\(\widehat{\beta}_1\)</span>s observed so far using a histogram. The idea here is that for a large number of <span class="math inline">\(\widehat{\beta}_1\)</span>s, the histogram gives a good approximation of the sampling distribution of the estimator. By decreasing the time between two sampling iterations, it becomes clear that the shape of the histogram approaches the characteristic bell shape of a normal distribution centered at the true slope of <span class="math inline">\(3\)</span>.</p>
<iframe height="470" width="700" frameborder="0" scrolling="no" src="SmallSampleDIstReg.html">
</iframe>
<p>(Double-click on the histogram to restart the simulation.)</p>
</p>
</div>
<div id="simulation-study-1" class="section level3 unnumbered hasAnchor">
<h3>Simulation Study 1<a href="lrwor.html#simulation-study-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Whether the statements of Key Concept 4.4 really hold can also be verified using <tt>R</tt>. For this we first build our own population of <span class="math inline">\(100000\)</span> observations in total. To do this we need values for the independent variable <span class="math inline">\(X\)</span>, for the error term <span class="math inline">\(u\)</span>, and for the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. With these combined in a simple regression model, we compute the dependent variable <span class="math inline">\(Y\)</span>. <br> In our example we generate the numbers <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i = 1\)</span>, … ,<span class="math inline">\(100000\)</span> by drawing a random sample from a uniform distribution on the interval <span class="math inline">\([0,20]\)</span>. The realizations of the error terms <span class="math inline">\(u_i\)</span> are drawn from a standard normal distribution with parameters <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 100\)</span> (note that <tt>rnorm()</tt> requires <span class="math inline">\(\sigma\)</span> as input for the argument <tt>sd</tt>, see <tt>?rnorm</tt>). Furthermore we chose <span class="math inline">\(\beta_0 = -2\)</span> and <span class="math inline">\(\beta_1 = 3.5\)</span> so the true model is</p>
<p><span class="math display">\[ Y_i = -2 + 3.5 \cdot X_i. \]</span></p>
<p>Finally, we store the results in a data.frame.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="lrwor.html#cb98-1" tabindex="-1"></a><span class="co"># simulate data</span></span>
<span id="cb98-2"><a href="lrwor.html#cb98-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb98-3"><a href="lrwor.html#cb98-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">runif</span>(N, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">20</span>)</span>
<span id="cb98-4"><a href="lrwor.html#cb98-4" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="at">sd =</span> <span class="dv">10</span>)</span>
<span id="cb98-5"><a href="lrwor.html#cb98-5" tabindex="-1"></a></span>
<span id="cb98-6"><a href="lrwor.html#cb98-6" tabindex="-1"></a><span class="co"># population regression</span></span>
<span id="cb98-7"><a href="lrwor.html#cb98-7" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">+</span> <span class="fl">3.5</span> <span class="sc">*</span> X <span class="sc">+</span> u</span>
<span id="cb98-8"><a href="lrwor.html#cb98-8" tabindex="-1"></a>population <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X, Y)</span></code></pre></div>
<p>From now on we will consider the previously generated data as the true population (which of course would be <em>unknown</em> in a real world application, otherwise there would be no reason to draw a random sample in the first place). The knowledge about the true population and the true relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> can be used to verify the statements made in Key Concept 4.4.</p>
<p>First, let us calculate the true variances <span class="math inline">\(\sigma^2_{\hat{\beta}_0}\)</span> and <span class="math inline">\(\sigma^2_{\hat{\beta}_1}\)</span> for a randomly drawn sample of size <span class="math inline">\(n = 100\)</span>.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="lrwor.html#cb99-1" tabindex="-1"></a><span class="co"># set sample size</span></span>
<span id="cb99-2"><a href="lrwor.html#cb99-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb99-3"><a href="lrwor.html#cb99-3" tabindex="-1"></a></span>
<span id="cb99-4"><a href="lrwor.html#cb99-4" tabindex="-1"></a><span class="co"># compute the variance of beta_hat_0</span></span>
<span id="cb99-5"><a href="lrwor.html#cb99-5" tabindex="-1"></a>H_i <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(X) <span class="sc">/</span> <span class="fu">mean</span>(X<span class="sc">^</span><span class="dv">2</span>) <span class="sc">*</span> X</span>
<span id="cb99-6"><a href="lrwor.html#cb99-6" tabindex="-1"></a>var_b0 <span class="ot">&lt;-</span> <span class="fu">var</span>(H_i <span class="sc">*</span> u) <span class="sc">/</span> (n <span class="sc">*</span> <span class="fu">mean</span>(H_i<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span> )</span>
<span id="cb99-7"><a href="lrwor.html#cb99-7" tabindex="-1"></a></span>
<span id="cb99-8"><a href="lrwor.html#cb99-8" tabindex="-1"></a><span class="co"># compute the variance of hat_beta_1</span></span>
<span id="cb99-9"><a href="lrwor.html#cb99-9" tabindex="-1"></a>var_b1 <span class="ot">&lt;-</span> <span class="fu">var</span>( ( X <span class="sc">-</span> <span class="fu">mean</span>(X) ) <span class="sc">*</span> u ) <span class="sc">/</span> (n <span class="sc">*</span> <span class="fu">var</span>(X)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="lrwor.html#cb100-1" tabindex="-1"></a><span class="co"># print variances to the console</span></span>
<span id="cb100-2"><a href="lrwor.html#cb100-2" tabindex="-1"></a>var_b0</span>
<span id="cb100-3"><a href="lrwor.html#cb100-3" tabindex="-1"></a><span class="co">#&gt; [1] 4.045066</span></span>
<span id="cb100-4"><a href="lrwor.html#cb100-4" tabindex="-1"></a>var_b1</span>
<span id="cb100-5"><a href="lrwor.html#cb100-5" tabindex="-1"></a><span class="co">#&gt; [1] 0.03018694</span></span></code></pre></div>
<p>Now let us assume that we do not know the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and that it is not possible to observe the whole population. However, we can observe a random sample of <span class="math inline">\(n\)</span> observations. Then, it would not be possible to compute the true parameters but we could obtain estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> from the sample data using OLS. However, we know that these estimates are outcomes of random variables themselves since the observations are randomly sampled from the population. Key Concept 4.4 describes their distributions for large <span class="math inline">\(n\)</span>. When drawing a single sample of size <span class="math inline">\(n\)</span> it is not possible to make any statement about these distributions. Things change if we repeat the sampling scheme many times and compute the estimates for each sample: using this procedure we simulate outcomes of the respective distributions.</p>
<p>To achieve this in R, we employ the following approach:</p>
<ul>
<li>We assign the number of repetitions, say <span class="math inline">\(10000\)</span>, to <tt>reps</tt> and then initialize a matrix <tt>fit</tt> where the estimates obtained in each sampling iteration shall be stored row-wise. Thus <tt>fit</tt> has to be a matrix of dimensions <tt>reps</tt><span class="math inline">\(\times2\)</span>.</li>
<li>In the next step we draw <tt>reps</tt> random samples of size <tt>n</tt> from the population and obtain the OLS estimates for each sample. The results are stored as row entries in the outcome matrix <tt>fit</tt>. This is done using a <tt>for()</tt> loop.</li>
<li>At last, we estimate variances of both estimators using the sampled outcomes and plot histograms of the latter. We also add a plot of the density functions belonging to the distributions that follow from Key Concept 4.4. The function <tt>bquote()</tt> is used to obtain math expressions in the titles and labels of both plots. See <tt>?bquote</tt>.</li>
</ul>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="lrwor.html#cb101-1" tabindex="-1"></a><span class="co"># set repetitions and sample size</span></span>
<span id="cb101-2"><a href="lrwor.html#cb101-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb101-3"><a href="lrwor.html#cb101-3" tabindex="-1"></a>reps <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb101-4"><a href="lrwor.html#cb101-4" tabindex="-1"></a></span>
<span id="cb101-5"><a href="lrwor.html#cb101-5" tabindex="-1"></a><span class="co"># initialize the matrix of outcomes</span></span>
<span id="cb101-6"><a href="lrwor.html#cb101-6" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> reps)</span>
<span id="cb101-7"><a href="lrwor.html#cb101-7" tabindex="-1"></a></span>
<span id="cb101-8"><a href="lrwor.html#cb101-8" tabindex="-1"></a><span class="co"># loop sampling and estimation of the coefficients</span></span>
<span id="cb101-9"><a href="lrwor.html#cb101-9" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>reps){</span>
<span id="cb101-10"><a href="lrwor.html#cb101-10" tabindex="-1"></a>  </span>
<span id="cb101-11"><a href="lrwor.html#cb101-11" tabindex="-1"></a> sample <span class="ot">&lt;-</span> population[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>N, n), ]</span>
<span id="cb101-12"><a href="lrwor.html#cb101-12" tabindex="-1"></a> fit[i, ] <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data =</span> sample)<span class="sc">$</span>coefficients</span>
<span id="cb101-13"><a href="lrwor.html#cb101-13" tabindex="-1"></a> </span>
<span id="cb101-14"><a href="lrwor.html#cb101-14" tabindex="-1"></a>}</span>
<span id="cb101-15"><a href="lrwor.html#cb101-15" tabindex="-1"></a></span>
<span id="cb101-16"><a href="lrwor.html#cb101-16" tabindex="-1"></a><span class="co"># compute variance estimates using outcomes</span></span>
<span id="cb101-17"><a href="lrwor.html#cb101-17" tabindex="-1"></a><span class="fu">var</span>(fit[, <span class="dv">1</span>])</span>
<span id="cb101-18"><a href="lrwor.html#cb101-18" tabindex="-1"></a><span class="co">#&gt; [1] 4.186832</span></span>
<span id="cb101-19"><a href="lrwor.html#cb101-19" tabindex="-1"></a><span class="fu">var</span>(fit[, <span class="dv">2</span>])</span>
<span id="cb101-20"><a href="lrwor.html#cb101-20" tabindex="-1"></a><span class="co">#&gt; [1] 0.03096199</span></span></code></pre></div>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="lrwor.html#cb102-1" tabindex="-1"></a><span class="co"># divide plotting area as 1-by-2 array</span></span>
<span id="cb102-2"><a href="lrwor.html#cb102-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb102-3"><a href="lrwor.html#cb102-3" tabindex="-1"></a></span>
<span id="cb102-4"><a href="lrwor.html#cb102-4" tabindex="-1"></a><span class="co"># plot histograms of beta_0 estimates</span></span>
<span id="cb102-5"><a href="lrwor.html#cb102-5" tabindex="-1"></a><span class="fu">hist</span>(fit[, <span class="dv">1</span>],</span>
<span id="cb102-6"><a href="lrwor.html#cb102-6" tabindex="-1"></a>     <span class="at">cex.main =</span> <span class="fl">0.8</span>,</span>
<span id="cb102-7"><a href="lrwor.html#cb102-7" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">bquote</span>(The <span class="sc">~</span> Distribution  <span class="sc">~</span> of <span class="sc">~</span> <span class="dv">10000</span> <span class="sc">~</span> beta[<span class="dv">0</span>] <span class="sc">~</span> Estimates), </span>
<span id="cb102-8"><a href="lrwor.html#cb102-8" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">bquote</span>(<span class="fu">hat</span>(beta)[<span class="dv">0</span>]), </span>
<span id="cb102-9"><a href="lrwor.html#cb102-9" tabindex="-1"></a>     <span class="at">freq =</span> F)</span>
<span id="cb102-10"><a href="lrwor.html#cb102-10" tabindex="-1"></a></span>
<span id="cb102-11"><a href="lrwor.html#cb102-11" tabindex="-1"></a><span class="co"># add true distribution to plot</span></span>
<span id="cb102-12"><a href="lrwor.html#cb102-12" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, </span>
<span id="cb102-13"><a href="lrwor.html#cb102-13" tabindex="-1"></a>            <span class="sc">-</span><span class="dv">2</span>, </span>
<span id="cb102-14"><a href="lrwor.html#cb102-14" tabindex="-1"></a>            <span class="fu">sqrt</span>(var_b0)), </span>
<span id="cb102-15"><a href="lrwor.html#cb102-15" tabindex="-1"></a>      <span class="at">add =</span> T, </span>
<span id="cb102-16"><a href="lrwor.html#cb102-16" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;darkred&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb102-17"><a href="lrwor.html#cb102-17" tabindex="-1"></a></span>
<span id="cb102-18"><a href="lrwor.html#cb102-18" tabindex="-1"></a><span class="co"># plot histograms of beta_hat_1 </span></span>
<span id="cb102-19"><a href="lrwor.html#cb102-19" tabindex="-1"></a><span class="fu">hist</span>(fit[, <span class="dv">2</span>],</span>
<span id="cb102-20"><a href="lrwor.html#cb102-20" tabindex="-1"></a>    <span class="at">cex.main =</span> <span class="fl">0.8</span>,</span>
<span id="cb102-21"><a href="lrwor.html#cb102-21" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">bquote</span>(The <span class="sc">~</span> Distribution  <span class="sc">~</span> of <span class="sc">~</span> <span class="dv">10000</span> <span class="sc">~</span> beta[<span class="dv">1</span>] <span class="sc">~</span> Estimates), </span>
<span id="cb102-22"><a href="lrwor.html#cb102-22" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">bquote</span>(<span class="fu">hat</span>(beta)[<span class="dv">1</span>]), </span>
<span id="cb102-23"><a href="lrwor.html#cb102-23" tabindex="-1"></a>     <span class="at">freq =</span> F)</span>
<span id="cb102-24"><a href="lrwor.html#cb102-24" tabindex="-1"></a></span>
<span id="cb102-25"><a href="lrwor.html#cb102-25" tabindex="-1"></a><span class="co"># add true distribution to plot</span></span>
<span id="cb102-26"><a href="lrwor.html#cb102-26" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, </span>
<span id="cb102-27"><a href="lrwor.html#cb102-27" tabindex="-1"></a>            <span class="fl">3.5</span>, </span>
<span id="cb102-28"><a href="lrwor.html#cb102-28" tabindex="-1"></a>            <span class="fu">sqrt</span>(var_b1)), </span>
<span id="cb102-29"><a href="lrwor.html#cb102-29" tabindex="-1"></a>      <span class="at">add =</span> T, </span>
<span id="cb102-30"><a href="lrwor.html#cb102-30" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;darkred&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-173-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Our variance estimates support the statements made in Key Concept 4.4, coming close to the theoretical values. The histograms suggest that the distributions of the estimators can be well approximated by the respective theoretical normal distributions stated in Key Concept 4.4.</p>
</div>
<div id="simulation-study-2" class="section level3 unnumbered hasAnchor">
<h3>Simulation Study 2<a href="lrwor.html#simulation-study-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A further result implied by Key Concept 4.4 is that both estimators are consistent, i.e., they converge in probability to the true parameters we are interested in. This is because they are asymptotically unbiased and their variances converge to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\)</span> increases. We can check this by repeating the simulation above for a sequence of increasing sample sizes. This means we no longer assign the sample size but a <em>vector</em> of sample sizes: <tt>n &lt;- c(…)</tt>. <br>
Let us look at the distributions of <span class="math inline">\(\beta_1\)</span>. The idea here is to add an additional call for <tt>for()</tt> to the code. This is done in order to loop over the vector of sample sizes <tt>n</tt>. For each of the sample sizes we carry out the same simulation as before but plot a density estimate for the outcomes of each iteration over <tt>n</tt>. Notice that we have to change <tt>n</tt> to <tt>n[j]</tt> in the inner loop to ensure that the <tt>j</tt><span class="math inline">\(^{th}\)</span> element of <tt>n</tt> is used. In the simulation, we use sample sizes of <span class="math inline">\(100, 250, 1000\)</span> and <span class="math inline">\(3000\)</span>. Consequently we have a total of four distinct simulations using different sample sizes.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="lrwor.html#cb103-1" tabindex="-1"></a><span class="co"># set seed for reproducibility</span></span>
<span id="cb103-2"><a href="lrwor.html#cb103-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb103-3"><a href="lrwor.html#cb103-3" tabindex="-1"></a></span>
<span id="cb103-4"><a href="lrwor.html#cb103-4" tabindex="-1"></a><span class="co"># set repetitions and the vector of sample sizes</span></span>
<span id="cb103-5"><a href="lrwor.html#cb103-5" tabindex="-1"></a>reps <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb103-6"><a href="lrwor.html#cb103-6" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">250</span>, <span class="dv">1000</span>, <span class="dv">3000</span>)</span>
<span id="cb103-7"><a href="lrwor.html#cb103-7" tabindex="-1"></a></span>
<span id="cb103-8"><a href="lrwor.html#cb103-8" tabindex="-1"></a><span class="co"># initialize the matrix of outcomes</span></span>
<span id="cb103-9"><a href="lrwor.html#cb103-9" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> reps)</span>
<span id="cb103-10"><a href="lrwor.html#cb103-10" tabindex="-1"></a></span>
<span id="cb103-11"><a href="lrwor.html#cb103-11" tabindex="-1"></a><span class="co"># divide the plot panel in a 2-by-2 array</span></span>
<span id="cb103-12"><a href="lrwor.html#cb103-12" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb103-13"><a href="lrwor.html#cb103-13" tabindex="-1"></a></span>
<span id="cb103-14"><a href="lrwor.html#cb103-14" tabindex="-1"></a><span class="co"># loop sampling and plotting</span></span>
<span id="cb103-15"><a href="lrwor.html#cb103-15" tabindex="-1"></a></span>
<span id="cb103-16"><a href="lrwor.html#cb103-16" tabindex="-1"></a><span class="co"># outer loop over n</span></span>
<span id="cb103-17"><a href="lrwor.html#cb103-17" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(n)) {</span>
<span id="cb103-18"><a href="lrwor.html#cb103-18" tabindex="-1"></a>  </span>
<span id="cb103-19"><a href="lrwor.html#cb103-19" tabindex="-1"></a>  <span class="co"># inner loop: sampling and estimating of the coefficients</span></span>
<span id="cb103-20"><a href="lrwor.html#cb103-20" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>reps){</span>
<span id="cb103-21"><a href="lrwor.html#cb103-21" tabindex="-1"></a>    </span>
<span id="cb103-22"><a href="lrwor.html#cb103-22" tabindex="-1"></a>    sample <span class="ot">&lt;-</span> population[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>N, n[j]), ]</span>
<span id="cb103-23"><a href="lrwor.html#cb103-23" tabindex="-1"></a>    fit[i, ] <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data =</span> sample)<span class="sc">$</span>coefficients</span>
<span id="cb103-24"><a href="lrwor.html#cb103-24" tabindex="-1"></a>    </span>
<span id="cb103-25"><a href="lrwor.html#cb103-25" tabindex="-1"></a>  }</span>
<span id="cb103-26"><a href="lrwor.html#cb103-26" tabindex="-1"></a>  </span>
<span id="cb103-27"><a href="lrwor.html#cb103-27" tabindex="-1"></a>  <span class="co"># draw density estimates</span></span>
<span id="cb103-28"><a href="lrwor.html#cb103-28" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="fu">density</span>(fit[ ,<span class="dv">2</span>]), <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">2.5</span>, <span class="fl">4.5</span>), </span>
<span id="cb103-29"><a href="lrwor.html#cb103-29" tabindex="-1"></a>       <span class="at">col =</span> j, </span>
<span id="cb103-30"><a href="lrwor.html#cb103-30" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">&quot;n=&quot;</span>, n[j]), </span>
<span id="cb103-31"><a href="lrwor.html#cb103-31" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="fu">bquote</span>(<span class="fu">hat</span>(beta)[<span class="dv">1</span>]))</span>
<span id="cb103-32"><a href="lrwor.html#cb103-32" tabindex="-1"></a>  </span>
<span id="cb103-33"><a href="lrwor.html#cb103-33" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-174-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>We find that, as <span class="math inline">\(n\)</span> increases, the distribution of <span class="math inline">\(\hat\beta_1\)</span> concentrates around its mean, i.e., its variance decreases. Put differently, the likelihood of observing estimates close to the true value of <span class="math inline">\(\beta_1 = 3.5\)</span> grows as we increase the sample size. The same behavior can be observed if we analyze the distribution of <span class="math inline">\(\hat\beta_0\)</span> instead.</p>
</div>
<div id="simulation-study-3" class="section level3 unnumbered hasAnchor">
<h3>Simulation Study 3<a href="lrwor.html#simulation-study-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Furthermore, (4.1) reveals that the variance of the OLS estimator for <span class="math inline">\(\beta_1\)</span> decreases as the variance of the <span class="math inline">\(X_i\)</span> increases. In other words, as we increase the amount of information provided by the regressor, that is, increasing <span class="math inline">\(Var(X)\)</span>, which is used to estimate <span class="math inline">\(\beta_1\)</span>, we become more confident that the estimate is close to the true value (i.e., <span class="math inline">\(Var(\hat\beta_1)\)</span> decreases).<br>
We can visualize this by reproducing Figure 4.6 from the book. To do this, we sample observations <span class="math inline">\((X_i,Y_i)\)</span>, <span class="math inline">\(i=1,\dots,100\)</span> from a bivariate normal distribution with</p>
<p><span class="math display">\[E(X)=E(Y)=5,\]</span>
<span class="math display">\[Var(X)=Var(Y)=5,\]</span>
and
<span class="math display">\[Cov(X,Y)=4.\]</span></p>
<p>Formally, this is written down as
<span class="math display">\[\begin{pmatrix} X \\ Y  \end{pmatrix}\overset{i.i.d.}{\sim} \ \mathcal{N}\left[\begin{pmatrix} 5 \\ 5  \end{pmatrix}, \begin{pmatrix} 5 &amp; 4 \\ 4 &amp; 5 \end{pmatrix} \right].\tag{4.3} \]</span></p>
<p>To carry out the random sampling, we make use of the function <tt>mvrnorm()</tt> from the package <tt>MASS</tt> <span class="citation">(<a href="#ref-R-MASS">Ripley 2023</a>)</span> which allows to draw random samples from multivariate normal distributions, see <code>?mvtnorm</code>. Next, we use <tt>subset()</tt> to split the sample into two subsets such that the first set, <tt>set1</tt>, consists of observations that fulfill the condition <span class="math inline">\(\lvert X - \overline{X} \rvert &gt; 1\)</span> and the second set, <tt>set2</tt>, includes the remainder of the sample. We then plot both sets and use different colors to distinguish the observations.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="lrwor.html#cb104-1" tabindex="-1"></a><span class="co"># load the MASS package</span></span>
<span id="cb104-2"><a href="lrwor.html#cb104-2" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb104-3"><a href="lrwor.html#cb104-3" tabindex="-1"></a></span>
<span id="cb104-4"><a href="lrwor.html#cb104-4" tabindex="-1"></a><span class="co"># set seed for reproducibility</span></span>
<span id="cb104-5"><a href="lrwor.html#cb104-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4</span>)</span>
<span id="cb104-6"><a href="lrwor.html#cb104-6" tabindex="-1"></a></span>
<span id="cb104-7"><a href="lrwor.html#cb104-7" tabindex="-1"></a><span class="co"># simulate bivariate normal data</span></span>
<span id="cb104-8"><a href="lrwor.html#cb104-8" tabindex="-1"></a>bvndata <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="dv">100</span>, </span>
<span id="cb104-9"><a href="lrwor.html#cb104-9" tabindex="-1"></a>                <span class="at">mu =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">5</span>), </span>
<span id="cb104-10"><a href="lrwor.html#cb104-10" tabindex="-1"></a>                <span class="at">Sigma =</span> <span class="fu">cbind</span>(<span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">4</span>), <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">5</span>))) </span>
<span id="cb104-11"><a href="lrwor.html#cb104-11" tabindex="-1"></a></span>
<span id="cb104-12"><a href="lrwor.html#cb104-12" tabindex="-1"></a><span class="co"># assign column names / convert to data.frame</span></span>
<span id="cb104-13"><a href="lrwor.html#cb104-13" tabindex="-1"></a><span class="fu">colnames</span>(bvndata) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;X&quot;</span>, <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb104-14"><a href="lrwor.html#cb104-14" tabindex="-1"></a>bvndata <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(bvndata)</span>
<span id="cb104-15"><a href="lrwor.html#cb104-15" tabindex="-1"></a></span>
<span id="cb104-16"><a href="lrwor.html#cb104-16" tabindex="-1"></a><span class="co"># subset the data</span></span>
<span id="cb104-17"><a href="lrwor.html#cb104-17" tabindex="-1"></a>set1 <span class="ot">&lt;-</span> <span class="fu">subset</span>(bvndata, <span class="fu">abs</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> X) <span class="sc">&gt;</span> <span class="dv">1</span>)</span>
<span id="cb104-18"><a href="lrwor.html#cb104-18" tabindex="-1"></a>set2 <span class="ot">&lt;-</span> <span class="fu">subset</span>(bvndata, <span class="fu">abs</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> X) <span class="sc">&lt;=</span> <span class="dv">1</span>)</span>
<span id="cb104-19"><a href="lrwor.html#cb104-19" tabindex="-1"></a></span>
<span id="cb104-20"><a href="lrwor.html#cb104-20" tabindex="-1"></a><span class="co"># plot both data sets</span></span>
<span id="cb104-21"><a href="lrwor.html#cb104-21" tabindex="-1"></a><span class="fu">plot</span>(set1, </span>
<span id="cb104-22"><a href="lrwor.html#cb104-22" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, </span>
<span id="cb104-23"><a href="lrwor.html#cb104-23" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Y&quot;</span>, </span>
<span id="cb104-24"><a href="lrwor.html#cb104-24" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb104-25"><a href="lrwor.html#cb104-25" tabindex="-1"></a></span>
<span id="cb104-26"><a href="lrwor.html#cb104-26" tabindex="-1"></a><span class="fu">points</span>(set2, </span>
<span id="cb104-27"><a href="lrwor.html#cb104-27" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>, </span>
<span id="cb104-28"><a href="lrwor.html#cb104-28" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb104-29"><a href="lrwor.html#cb104-29" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, </span>
<span id="cb104-30"><a href="lrwor.html#cb104-30" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Set1&quot;</span>, </span>
<span id="cb104-31"><a href="lrwor.html#cb104-31" tabindex="-1"></a>                  <span class="st">&quot;Set2&quot;</span>),</span>
<span id="cb104-32"><a href="lrwor.html#cb104-32" tabindex="-1"></a>       <span class="at">cex =</span> <span class="dv">1</span>,</span>
<span id="cb104-33"><a href="lrwor.html#cb104-33" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb104-34"><a href="lrwor.html#cb104-34" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;steelblue&quot;</span>))</span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-175-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>It is clear that observations that are close to the sample average of the <span class="math inline">\(X_i\)</span> have less variance than those that are farther away. Now, if we were to draw a line as accurately as possible through either of the two sets it is intuitive that choosing the observations indicated by the black dots, i.e., using the set of observations which has larger variance than the blue ones, would result in a more precise line. Now, let us use OLS to estimate slope and intercept for both sets of observations. We then plot the observations along with both regression lines.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="lrwor.html#cb105-1" tabindex="-1"></a><span class="co"># estimate both regression lines</span></span>
<span id="cb105-2"><a href="lrwor.html#cb105-2" tabindex="-1"></a>lm.set1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data =</span> set1)</span>
<span id="cb105-3"><a href="lrwor.html#cb105-3" tabindex="-1"></a>lm.set2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data =</span> set2)</span>
<span id="cb105-4"><a href="lrwor.html#cb105-4" tabindex="-1"></a></span>
<span id="cb105-5"><a href="lrwor.html#cb105-5" tabindex="-1"></a><span class="co"># plot observations</span></span>
<span id="cb105-6"><a href="lrwor.html#cb105-6" tabindex="-1"></a><span class="fu">plot</span>(set1, <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Y&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb105-7"><a href="lrwor.html#cb105-7" tabindex="-1"></a><span class="fu">points</span>(set2, <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb105-8"><a href="lrwor.html#cb105-8" tabindex="-1"></a></span>
<span id="cb105-9"><a href="lrwor.html#cb105-9" tabindex="-1"></a><span class="co"># add both lines to the plot</span></span>
<span id="cb105-10"><a href="lrwor.html#cb105-10" tabindex="-1"></a><span class="fu">abline</span>(lm.set1, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb105-11"><a href="lrwor.html#cb105-11" tabindex="-1"></a><span class="fu">abline</span>(lm.set2, <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb105-12"><a href="lrwor.html#cb105-12" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, </span>
<span id="cb105-13"><a href="lrwor.html#cb105-13" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Set1&quot;</span>, </span>
<span id="cb105-14"><a href="lrwor.html#cb105-14" tabindex="-1"></a>                  <span class="st">&quot;Set2&quot;</span>),</span>
<span id="cb105-15"><a href="lrwor.html#cb105-15" tabindex="-1"></a>       <span class="at">cex =</span> <span class="dv">1</span>,</span>
<span id="cb105-16"><a href="lrwor.html#cb105-16" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb105-17"><a href="lrwor.html#cb105-17" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;steelblue&quot;</span>))</span></code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-176-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Evidently, the green regression line does far better in describing data sampled from the bivariate normal distribution stated in (<a href="#mjx-eqn-4.3">4.3</a>) than the red line. This is a nice example for demonstrating why we are interested in a high variance of the regressor <span class="math inline">\(X\)</span>: more variance in the <span class="math inline">\(X_i\)</span> means more information from which the precision of the estimation benefits.</p>
</div>
</div>
<div id="exercises-4" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Exercises<a href="lrwor.html#exercises-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="class-sizes-and-test-scores" class="section level4 unnumbered hasAnchor DCexercise">
<h4>1. Class Sizes and Test Scores<a href="lrwor.html#class-sizes-and-test-scores" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A researcher wants to analyze the relationship between class size (measured by the student-teacher ratio) and the average test score. Therefore he measures both variables in <span class="math inline">\(10\)</span> different classes and ends up with the following results.</p>


  <table>
      <tr>
        <td><b>Class Size</b></td>
        <td>23</td>
        <td>19</td>
        <td>30</td>
        <td>22</td>
        <td>23</td>
        <td>29</td>
        <td>35</td>
        <td>36</td>
        <td>33</td>
        <td>25</td>
      </tr>
      <tr>
        <td><b>Test Score</b></td>
        <td>430</td>
        <td>430</td>
        <td>333</td>
        <td>410</td>
        <td>390</td>
        <td>377</td>
        <td>325</td>
        <td>310</td>
        <td>328</td>
        <td>375</td>
      </tr>
    </table>


<p><strong>Instructions:</strong></p>
<ul>
<li><p>Create the vectors <tt>cs</tt> (the class size) and <tt>ts</tt> (the test score), containing the observations above.</p></li>
<li><p>Draw a scatterplot of the results using <tt>plot()</tt>.</p></li>
</ul>
<iframe src="DCL/ex4_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div id="mean-variance-covariance-and-correlation" class="section level4 unnumbered hasAnchor DCexercise">
<h4>2. Mean, Variance, Covariance and Correlation<a href="lrwor.html#mean-variance-covariance-and-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The vectors <tt>cs</tt> and <tt>ts</tt> are available in the working environment (you can check this: type their names into the console and press enter).</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Compute the mean, the sample variance and the sample standard deviation of <tt>ts</tt>.</p></li>
<li><p>Compute the covariance and the correlation coefficient for <tt>ts</tt> and <tt>cs</tt>.</p></li>
</ul>
<iframe src="DCL/ex4_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong> Use the <tt>R</tt> functions presented in this chapter: <tt>mean()</tt>, <tt>sd()</tt>, <tt>cov()</tt>, <tt>cor()</tt> and <tt>var()</tt>.</p>
</div>
<div id="simple-linear-regression-1" class="section level4 unnumbered hasAnchor DCexercise">
<h4>3. Simple Linear Regression<a href="lrwor.html#simple-linear-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The vectors <tt>cs</tt> and <tt>ts</tt> are available in the working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>The function <tt>lm()</tt> is part of the package <tt>AER</tt>. Attach the package using <tt>library()</tt>.</p></li>
<li><p>Use <tt>lm()</tt> to estimate the regression model <span class="math display">\[TestScore_i = \beta_0 + \beta_1 STR_i + u_i.\]</span> Assign the result to <tt>mod</tt>.</p></li>
<li><p>Obtain a statistical summary of the model.</p></li>
</ul>
<iframe src="DCL/ex4_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div id="the-model-object" class="section level4 unnumbered hasAnchor DCexercise">
<h4>4. The Model Object<a href="lrwor.html#the-model-object" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let us see how an object of class <tt>lm</tt> is structured.</p>
<p>The vectors <tt>cs</tt> and <tt>ts</tt> as well as the model object <tt>mod</tt> from the previous exercise are available in your workspace.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Use <tt>class()</tt> to learn about the class of the object <tt>mod</tt>.</li>
<li><tt>mod</tt> is an object of type <tt>list</tt> with named entries. Check this using the function <tt>is.list()</tt>.</li>
<li>See what information you can obtain from <tt>mod</tt> using <tt>names()</tt>.</li>
<li>Read out an arbitrary entry of the object <tt>mod</tt> using the <tt>$</tt> operator.</li>
</ul>
<iframe src="DCL/ex4_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div id="plotting-the-regression-line" class="section level4 unnumbered hasAnchor DCexercise">
<h4>5. Plotting the Regression Line<a href="lrwor.html#plotting-the-regression-line" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>You are provided with the code for the scatterplot in <tt>script.R</tt></p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Add the regression line to the scatterplot from a few exercises before.</p></li>
<li><p>The object <tt>mod</tt> is available in your working environment.</p></li>
</ul>
<iframe src="DCL/ex4_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong> Use the function <tt>abline()</tt>.</p>
</div>
<div id="summary-of-a-model-object" class="section level4 unnumbered hasAnchor DCexercise">
<h4>6. Summary of a Model Object<a href="lrwor.html#summary-of-a-model-object" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now read out and store some of the information that is contained in the output of <tt>summary()</tt>.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Assign the output of <tt>summary(mod)</tt> to the variable <tt>s</tt>.</p></li>
<li><p>Check entry names of the object <tt>s</tt>.</p></li>
<li><p>Create a new variable <tt>R2</tt> and assign the <span class="math inline">\(R^2\)</span> of the regression.</p></li>
</ul>
<p>The object <tt>mod</tt> is available in your working environment.</p>
<iframe src="DCL/ex4_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div id="estimated-coefficients" class="section level4 unnumbered hasAnchor DCexercise">
<h4>7. Estimated Coefficients<a href="lrwor.html#estimated-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The function <tt>summary()</tt> also provides information on the statistical significance of the estimated coefficients.</p>
<p><strong>Instructions:</strong></p>
<p>Extract the named <span class="math inline">\(2\times4\)</span> matrix with estimated coefficients, standard errors, <span class="math inline">\(t\)</span>-statistics and corresponding <span class="math inline">\(p\)</span>-values from the model summary <tt>s</tt>. Save this matrix in an object named <tt>coefs</tt>.</p>
<p>The objects <tt>mod</tt> and <tt>s</tt> are available in your working environment.</p>
<iframe src="DCL/ex4_7.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div id="dropping-the-intercept" class="section level4 unnumbered hasAnchor DCexercise">
<h4>8. Dropping the Intercept<a href="lrwor.html#dropping-the-intercept" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>So far, we have estimated regression models consisting of an intercept and a single regressor. In this exercise you will learn how to specify and how to estimate regression a model without intercept.</p>
<p>Note that excluding the intercept from a regression model might be a dodgy practice in some applications as this imposes the conditional expectation function of the dependent variable to be zero if the regressor is zero.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Figure out how the <tt>formula</tt> argument must be specified for a regression of <tt>ts</tt> solely on <tt>cs</tt>, i.e., a regression without intercept. Google is your friend!</p></li>
<li><p>Estimate the regression model without intercept and store the result in <tt>mod_ni</tt>.</p></li>
</ul>
<p>The vectors <tt>cs</tt>, <tt>ts</tt> and the model object <tt>mod</tt> from previous exercises are available in the working environment.</p>
<iframe src="DCL/ex4_8.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div id="regression-output-no-constant-case" class="section level4 unnumbered hasAnchor DCexercise">
<h4>9. Regression Output: No Constant Case<a href="lrwor.html#regression-output-no-constant-case" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In Exercise 8 you have estimated a model without intercept. The estimated regression function is</p>
<p><span class="math display">\[\widehat{TestScore} = \underset{(1.36)}{12.65} \times STR.\]</span></p>
<p><strong>Instructions:</strong></p>
<p>Convince yourself that everything is as stated above: extract the coefficient matrix from the summary of <tt>mod_ni</tt> and store it in a variable named <tt>coef</tt>.</p>
<p>The vectors <tt>cs</tt>, <tt>ts</tt> as well as the model object <tt>mod_ni</tt> from the previous exercise are available in your working environment.</p>
<iframe src="DCL/ex4_9.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong> An entry of a named list can be accessed using the <tt>$</tt> operator.</p>
</div>
<div id="regression-output-no-constant-case-ctd." class="section level4 unnumbered hasAnchor DCexercise">
<h4>10. Regression Output: No Constant Case — Ctd.<a href="lrwor.html#regression-output-no-constant-case-ctd." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In Exercises 8 and 9 you have dealt with a model without intercept. The estimated regression function was</p>
<p><span class="math display">\[\widehat{TestScore_i} = \underset{(1.36)}{12.65} \times STR_i.\]</span></p>
<p>The coefficient matrix <tt>coef</tt> from Exercise 9 contains the estimated coefficient on <span class="math inline">\(STR\)</span>, its standard error, the <span class="math inline">\(t\)</span>-statistic of the significance test and the corresponding <span class="math inline">\(p\)</span>-value.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Print the contents of <tt>coef</tt> to the console.</li>
<li>Convince yourself that the reported <span class="math inline">\(t\)</span>-statistic is correct: use the entries of <tt>coef</tt> to compute the <span class="math inline">\(t\)</span>-statistic and save it to <tt>t_stat</tt>.</li>
</ul>
<p>The matrix <tt>coef</tt> from the previous exercise is available in your working environment.</p>
<iframe src="DCL/ex4_10.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p><tt>X[a,b]</tt> returns the <tt>[a,b]</tt> element of the matrix <tt>X</tt>.</p></li>
<li><p>The <span class="math inline">\(t\)</span>-statistic for a test of the hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> is computed as <span class="math display">\[t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}.\]</span></p></li>
</ul>
</div>
<div id="two-regressions-one-plot" class="section level4 unnumbered hasAnchor DCexercise">
<h4>11. Two Regressions, One Plot<a href="lrwor.html#two-regressions-one-plot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The two estimated regression models from the previous exercises are</p>
<p><span class="math display">\[\widehat{TestScore_i} = \underset{(1.36)}{12.65} \times STR_i\]</span></p>
<p>and</p>
<p><span class="math display">\[\widehat{TestScore_i} = \underset{(23.96)}{567.4272} \underset{(0.85)}{-7.1501} \times STR_i.\]</span></p>
<p>You are provided with the code line <tt>plot(cs, ts)</tt> which creates a scatterplot of <tt>ts</tt> and <tt>cs</tt>. Note that this line must be executed before calling <tt>abline()</tt>! You may color the regression lines by using, e.g., <tt>col = “red”</tt> or <tt>col = “blue”</tt> as an additional argument to <tt>abline()</tt> for better distinguishability.</p>
<p>The vectors <tt>cs</tt> and <tt>ts</tt> as well as the list objects <tt>mod</tt> and <tt>mod_ni</tt> from previous exercises are available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<p>Generate a scatterplot of <tt>ts</tt> and <tt>cs</tt> and add the estimated regression lines of <tt>mod</tt> and <tt>mod_ni</tt>.</p>
<iframe src="DCL/ex4_11.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div id="tss-and-ssr" class="section level4 unnumbered hasAnchor DCexercise">
<h4>12. <span class="math inline">\(TSS\)</span> and <span class="math inline">\(SSR\)</span><a href="lrwor.html#tss-and-ssr" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If graphical inspection does not help, researchers resort to analytic techniques in order to detect if a model fits the data at hand well or better than another model.</p>
<p>Let us go back to the simple regression model including an intercept. The estimated regression line for <tt>mod</tt> was</p>
<p><span class="math display">\[\widehat{TestScore_i} = 567.43 - 7.15 \times STR_i, \, R^2 = 0.8976, \, SER=15.19.\]</span></p>
<p>You can check this as <tt>mod</tt> and the vectors <tt>cs</tt> and <tt>ts</tt> are available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Compute <span class="math inline">\(SSR\)</span>, the sum of squared residuals, and save it to <tt>ssr</tt>.</li>
<li>Compute <span class="math inline">\(TSS\)</span>, the total sum of squares, and save it to <tt>tss</tt>.</li>
</ul>
<iframe src="DCL/ex4_12.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div id="the-r2-of-a-regression-model" class="section level4 unnumbered hasAnchor DCexercise">
<h4>13. The <span class="math inline">\(R^2\)</span> of a Regression Model<a href="lrwor.html#the-r2-of-a-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(R^2\)</span> of the regression saved in <tt>mod</tt> is <span class="math inline">\(0.8976\)</span>. You can check this by executing <tt>summary(mod)$r.squared</tt> in the console below.</p>
<p>Remember the formula of <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}\]</span></p>
<p>The objects <tt>mod</tt>, <tt>tss</tt> and <tt>ssr</tt> from the previous exercise are available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Use <tt>ssr</tt> and <tt>tss</tt> to compute <span class="math inline">\(R^2\)</span> manually. <em>Round</em> the result to <em>four</em> decimal places and save it to <tt>R2</tt>.</li>
<li>Use the logical operator <tt>==</tt> to check whether your result matches the value mentioned above.</li>
</ul>
<iframe src="DCL/ex4_13.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<p>You may round numeric values using the function <tt>round()</tt>.</p>
</div>
<div id="the-standard-error-of-the-regression-1" class="section level4 unnumbered hasAnchor DCexercise">
<h4>14. The Standard Error of The Regression<a href="lrwor.html#the-standard-error-of-the-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The standard error of the Regression in the simple regression model is <span class="math display">\[SER = \frac{1}{n-2} \sum_{i=1}^n \widehat{u}_i^2 =\sqrt{\frac{SSR}{n-2}}.\]</span> <span class="math inline">\(SER\)</span> measures the size of an average residual which is an estimate of the magnitude of a typical regression error.</p>
<p>The model object <tt>mod</tt> and the vectors <tt>cs</tt> and </tt>ts</tt> are available in your workspace.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Use <tt>summary()</tt> to obtain the <span class="math inline">\(SER\)</span> for the regression of <tt>ts</tt> on <tt>cs</tt> saved in the model object <tt>mod</tt>. Save the result in the variable <tt>SER</tt>.</p></li>
<li><p>Use <tt>SER</tt> to compute the <span class="math inline">\(SSR\)</span> and store it in <tt>SSR</tt>.</p></li>
<li><p>Check that <tt>SSR</tt> is indeed the <span class="math inline">\(SSR\)</span> by comparing <tt>SSR</tt> to the result of <tt>sum(mod$residuals^2)</tt></p></li>
</ul>
<iframe src="DCL/ex4_14.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div id="the-estimated-covariance-matrix" class="section level4 unnumbered hasAnchor DCexercise">
<h4>15. The Estimated Covariance Matrix<a href="lrwor.html#the-estimated-covariance-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As has been discussed in Chapter <a href="lrwor.html#tlsa">4.4</a>, the OLS estimators <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> are functions of the random error term. Therefore, they are random variables themselves. For two or more random variables, their covariances and variances are summarized by a <em>variance-covariance matrix</em> (which is often simply called the <em>covariance matrix</em>). Taking the square root of the diagonal elements of the estimated covariance matrix obtains <span class="math inline">\(SE(\widehat\beta_0)\)</span> and <span class="math inline">\(SE(\widehat\beta_1)\)</span>, the standard errors of <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>.</p>
<p><tt>summary()</tt> computes an estimate of this matrix. The respective entry in the output of summary (remember that <tt>summary()</tt> produces a list) is called <tt>cov.unscaled</tt>. The model object <tt>mod</tt> is available in your workspace.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Use <tt>summary()</tt> to obtain the covariance matrix estimate for the regression of test scores on student-teacher ratios stored in the model object <tt>mod</tt>. Save the result to <tt>cov_matrix</tt>.</p></li>
<li><p>Obtain the diagonal elements of <tt>cov_matrix</tt>, compute their square root and assign the result to the variable <tt>SEs</tt>.</p></li>
</ul>
<iframe src="DCL/ex4_15.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong> <tt>diag(A)</tt> returns a vector containing the diagonal elements of the matrix <tt>A</tt>.</p>
</div>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-R-AER" class="csl-entry">
Kleiber, Christian, and Achim Zeileis. 2008. <em>Applied Econometrics with <span>R</span></em>. New York: Springer-Verlag. <a href="https://CRAN.R-project.org/package=AER">https://CRAN.R-project.org/package=AER</a>.
</div>
<div id="ref-kleiber2008" class="csl-entry">
Kleiber, C., and A. Zeileis. 2008. <em><span>A</span>pplied <span>E</span>conometrics with <span>R</span></em>. <span>S</span>pringer.
</div>
<div id="ref-R-MASS" class="csl-entry">
Ripley, Brian. 2023. <em><span class="nocase">MASS: Support Functions and Datasets for Venables and Ripley’s MASS</span></em> (version 7.3-60). <a href="http://www.stats.ox.ac.uk/pub/MASS4/">http://www.stats.ox.ac.uk/pub/MASS4/</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>See Chapter <a href="ittsraf.html#ittsraf">14</a> for more on autoregressive processes and time series analysis in general.<a href="lrwor.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>See Chapter 4.4 of the book.<a href="lrwor.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="arosur.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="htaciitslrm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mca91/EconometricsWithR/edit/master/04-ch4.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
